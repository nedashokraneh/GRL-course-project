{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c82a59da",
   "metadata": {},
   "source": [
    "# Set up LogME for Strategies for Pre-training GNNs (Hu et al., ICLR 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d1eb90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "\n",
    "from loader import MoleculeDataset\n",
    "from model import GNN, GNN_graphpred\n",
    "from splitters import scaffold_split\n",
    "\n",
    "\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26536f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe_save_to_csv(embeddings, labels, dataset_name, model_name, save_path):\n",
    "#     ## create pandas dataframe to store: example id, embeddings, labels ## \n",
    "#     d = {'example_id': [i for i in range(len(embeddings))],\n",
    "#             'embeddings': embeddings,\n",
    "#             'labels': labels\n",
    "#            }\n",
    "#     df = pd.DataFrame(data=d)\n",
    "#     # df.head(15)\n",
    "\n",
    "#     if not os.path.exists(save_path):\n",
    "#         os.makedirs(save_path)\n",
    "\n",
    "    filename = '{}_{}.csv'.format(dataset_name, model_name)\n",
    "#     print(\"dataset_name: {}\".format(dataset_name))\n",
    "#     df.to_csv(os.path.join(save_path, filename), index=False)\n",
    "    emb_df = pd.DataFrame(np.array(embeddings))\n",
    "    emb_df.columns = ['emb' + str(e+1) for e in range(emb_df.shape[1])]\n",
    "    emb_df['label'] = labels\n",
    "    print(\"emb_df: \\n{}\\n\".format(emb_df))\n",
    "    emb_df.to_csv(os.path.join(save_path, filename), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd155f5c",
   "metadata": {},
   "source": [
    "## Load Pre-trained GNN models (Hu et al., ICLR 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e817504f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GNN_graphpred(\n",
       "  (gnn): GNN(\n",
       "    (x_embedding1): Embedding(120, 300)\n",
       "    (x_embedding2): Embedding(3, 300)\n",
       "    (gnns): ModuleList(\n",
       "      (0): GINConv(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=600, out_features=300, bias=True)\n",
       "        )\n",
       "        (edge_embedding1): Embedding(6, 300)\n",
       "        (edge_embedding2): Embedding(3, 300)\n",
       "      )\n",
       "      (1): GINConv(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=600, out_features=300, bias=True)\n",
       "        )\n",
       "        (edge_embedding1): Embedding(6, 300)\n",
       "        (edge_embedding2): Embedding(3, 300)\n",
       "      )\n",
       "      (2): GINConv(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=600, out_features=300, bias=True)\n",
       "        )\n",
       "        (edge_embedding1): Embedding(6, 300)\n",
       "        (edge_embedding2): Embedding(3, 300)\n",
       "      )\n",
       "      (3): GINConv(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=600, out_features=300, bias=True)\n",
       "        )\n",
       "        (edge_embedding1): Embedding(6, 300)\n",
       "        (edge_embedding2): Embedding(3, 300)\n",
       "      )\n",
       "      (4): GINConv(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=600, out_features=300, bias=True)\n",
       "        )\n",
       "        (edge_embedding1): Embedding(6, 300)\n",
       "        (edge_embedding2): Embedding(3, 300)\n",
       "      )\n",
       "    )\n",
       "    (batch_norms): ModuleList(\n",
       "      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tasks = 1\n",
    "\n",
    "num_layer = 5 # default\n",
    "emb_dim = 300 # default\n",
    "JK = 'last' # default (how the node features across laysers are combined)\n",
    "dropout_ratio = 0.5 # default\n",
    "graph_pooling = 'mean' # default\n",
    "gnn_type = 'gin' # default\n",
    "\n",
    "##########################\n",
    "input_model_file = './model_gin/supervised.pth'\n",
    "\n",
    "gin_supervised_model = GNN_graphpred(num_layer, emb_dim, num_tasks, JK = JK, drop_ratio = dropout_ratio, graph_pooling = graph_pooling, gnn_type = gnn_type)\n",
    "gin_supervised_model.from_pretrained(input_model_file)\n",
    "\n",
    "gin_supervised_model.to(device)\n",
    "gin_supervised_model.eval()\n",
    "\n",
    "###########################\n",
    "\n",
    "input_model_file = './model_gin/supervised_infomax.pth'\n",
    "\n",
    "gin_supervised_infomax_model = GNN_graphpred(num_layer, emb_dim, num_tasks, JK = JK, drop_ratio = dropout_ratio, graph_pooling = graph_pooling, gnn_type = gnn_type)\n",
    "gin_supervised_infomax_model.from_pretrained(input_model_file)\n",
    "\n",
    "gin_supervised_infomax_model.to(device)\n",
    "gin_supervised_infomax_model.eval()\n",
    "\n",
    "###########################\n",
    "\n",
    "input_model_file = './model_gin/supervised_edgepred.pth'\n",
    "\n",
    "gin_supervised_edgepred_model = GNN_graphpred(num_layer, emb_dim, num_tasks, JK = JK, drop_ratio = dropout_ratio, graph_pooling = graph_pooling, gnn_type = gnn_type)\n",
    "gin_supervised_edgepred_model.from_pretrained(input_model_file)\n",
    "\n",
    "gin_supervised_edgepred_model.to(device)\n",
    "gin_supervised_edgepred_model.eval()\n",
    "\n",
    "###########################\n",
    "\n",
    "input_model_file = './model_gin/supervised_masking.pth'\n",
    "\n",
    "gin_supervised_masking_model = GNN_graphpred(num_layer, emb_dim, num_tasks, JK = JK, drop_ratio = dropout_ratio, graph_pooling = graph_pooling, gnn_type = gnn_type)\n",
    "gin_supervised_masking_model.from_pretrained(input_model_file)\n",
    "\n",
    "gin_supervised_masking_model.to(device)\n",
    "gin_supervised_masking_model.eval()\n",
    "\n",
    "###########################\n",
    "\n",
    "input_model_file = './model_gin/supervised_contextpred.pth'\n",
    "\n",
    "gin_supervised_contextpred_model = GNN_graphpred(num_layer, emb_dim, num_tasks, JK = JK, drop_ratio = dropout_ratio, graph_pooling = graph_pooling, gnn_type = gnn_type)\n",
    "gin_supervised_contextpred_model.from_pretrained(input_model_file)\n",
    "\n",
    "gin_supervised_contextpred_model.to(device)\n",
    "gin_supervised_contextpred_model.eval()\n",
    "\n",
    "###########################\n",
    "\n",
    "input_model_file = './model_gin/infomax.pth'\n",
    "\n",
    "gin_infomax_model = GNN_graphpred(num_layer, emb_dim, num_tasks, JK = JK, drop_ratio = dropout_ratio, graph_pooling = graph_pooling, gnn_type = gnn_type)\n",
    "gin_infomax_model.from_pretrained(input_model_file)\n",
    "\n",
    "gin_infomax_model.to(device)\n",
    "gin_infomax_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a766422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_features_labels(loader, model, seed):\n",
    "    \"\"\"Extract graph features.\n",
    "    Args:\n",
    "        loader: graph dataloader\n",
    "        model: feature extractor\n",
    "        seed: integer value for setting random seed\n",
    "    Returns:\n",
    "        all_graph_features: list of all graph features from the dataloader \n",
    "        all_graph_labels: list of all graph (true) labels from the dataloader\n",
    "    \"\"\"\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    all_graph_features = []\n",
    "    all_graph_labels = []\n",
    "    for step, batch in enumerate(loader):\n",
    "        batch = batch.to(device)\n",
    "        x = batch.x\n",
    "        edge_index = batch.edge_index\n",
    "        edge_attr = batch.edge_attr\n",
    "        y = batch.y\n",
    "        batch = batch.batch\n",
    "\n",
    "        # code from GNN_graphpred.forward() #\n",
    "        node_representation = model.gnn(x, edge_index, edge_attr)\n",
    "#         print(\"step {} | node_representation.shape: {}\".format(step, node_representation.shape))\n",
    "\n",
    "        graph_features = model.pool(node_representation, batch)\n",
    "#         print(\"graph feature (shape): {}\".format(graph_features.shape))\n",
    "        all_graph_features.extend(graph_features.cpu().detach().numpy())\n",
    "#         print(\"len(all_graph_features): {}\".format(len(all_graph_features)))\n",
    "\n",
    "        graph_preds = model.graph_pred_linear(graph_features)\n",
    "    #     print(\"graph predictions: {}\".format(graph_preds))\n",
    "    #     print(\"graph predictions (shape): {}\".format(graph_preds.shape))\n",
    "    #     y_true = y.view(graph_preds.shape)\n",
    "    #     print(\"y true: {}\".format(y_true))\n",
    "    #     print(\"y true (shape): {}\".format(y_true.shape))\n",
    "    #     print(\"y (shape): {}\".format(y.shape))\n",
    "\n",
    "        all_graph_labels.extend(y.cpu().detach().numpy())\n",
    "#         print(\"len(all_graph_labels): {}\".format(len(all_graph_labels)))\n",
    "        \n",
    "    return all_graph_features, all_graph_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1255bfe5",
   "metadata": {},
   "source": [
    "## Extract features of BBBP training set using pre-trained GNN & obtain LogME score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c238b6a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoleculeDataset(2039)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:52:51] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:52:51] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:52:51] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:52:51] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:52:51] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:52:51] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:52:51] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:52:51] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:52:51] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:52:51] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:52:51] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:52:51] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:52:51] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:52:51] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:52:51] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:52:52] Conflicting single bond directions around double bond at index 1.\n",
      "[17:52:52]   BondStereo set to STEREONONE and single bond directions set to NONE.\n",
      "[17:52:52] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:52:52] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:52:52] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:52:52] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:52:52] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:52:52] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:52:52] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:52:52] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:52:52] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:52:52] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:52:52] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:52:52] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:52:52] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:52:52] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:52:52] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:52:52] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:52:52] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:52:52] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:52:52] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:52:52] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:52:52] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:52:52] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:52:52] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:52:52] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:52:52] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_attr=[46, 2], edge_index=[2, 46], id=[1], x=[23, 2], y=[1])\n",
      "emb_df: \n",
      "          emb1      emb2      emb3      emb4      emb5      emb6      emb7  \\\n",
      "0     0.187954  0.013389 -0.250219 -0.022584  0.086127 -0.006174 -0.109610   \n",
      "1    -0.043880  0.003614 -0.058039  0.023666 -0.031128 -0.049535  0.085190   \n",
      "2     0.065282  0.006262 -0.139585 -0.201065 -0.003127 -0.061996 -0.065256   \n",
      "3    -0.070854  0.009151 -0.046193  0.016986 -0.002188 -0.034686  0.017967   \n",
      "4    -0.002685  0.000574 -0.042318  0.043741  0.093193  0.027315 -0.036415   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "1626  0.103538  0.009691  0.052273 -0.013520 -0.059032 -0.062254 -0.006430   \n",
      "1627  0.142405  0.015304  0.013123 -0.097107 -0.086527 -0.128479  0.016773   \n",
      "1628  0.153705  0.009646 -0.035269  0.018985  0.021211 -0.117989 -0.053596   \n",
      "1629  0.132085  0.007505 -0.094936 -0.010299  0.063251 -0.106890  0.022236   \n",
      "1630 -0.088223  0.027770  0.030527  0.013800  0.025591 -0.001884 -0.002569   \n",
      "\n",
      "          emb8      emb9     emb10  ...    emb292    emb293    emb294  \\\n",
      "0    -0.022883 -0.058373  0.037311  ...  0.005311  0.020930 -0.060527   \n",
      "1    -0.016493 -0.097433 -0.084806  ... -0.018109  0.094439  0.105347   \n",
      "2     0.003496 -0.057666 -0.156339  ... -0.005814  0.063931 -0.039143   \n",
      "3    -0.005076 -0.117960 -0.111846  ... -0.016731  0.031047  0.137751   \n",
      "4    -0.099702 -0.058213  0.002236  ... -0.011616 -0.087026  0.013834   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "1626 -0.021354 -0.064353  0.033381  ... -0.009606  0.143305  0.052681   \n",
      "1627 -0.083921 -0.234405  0.022100  ...  0.003596  0.000065 -0.052889   \n",
      "1628 -0.038685 -0.153656  0.003421  ... -0.006952  0.087719  0.035054   \n",
      "1629  0.109091 -0.055066  0.102262  ... -0.003924  0.094972  0.045096   \n",
      "1630 -0.022027 -0.070830  0.073275  ...  0.005907  0.059549 -0.000009   \n",
      "\n",
      "        emb295    emb296    emb297    emb298    emb299    emb300  label  \n",
      "0     0.006696 -0.086793  0.018117  0.004106  0.030810 -0.025228      1  \n",
      "1    -0.024139  0.079557  0.018468 -0.027128  0.124299 -0.024505      1  \n",
      "2     0.063861 -0.162414  0.061115 -0.079094  0.100402 -0.024895      1  \n",
      "3    -0.049587  0.111952  0.053997 -0.029490  0.087801 -0.025185      1  \n",
      "4    -0.092309 -0.043451  0.029909  0.151858  0.140708 -0.024558      1  \n",
      "...        ...       ...       ...       ...       ...       ...    ...  \n",
      "1626 -0.063997  0.079292  0.004547  0.000766 -0.017463 -0.024525      1  \n",
      "1627  0.012218 -0.026309  0.060815  0.016543  0.032542 -0.025266      1  \n",
      "1628 -0.044030  0.059293 -0.078783  0.017062 -0.068424 -0.025041      1  \n",
      "1629 -0.059087  0.079798 -0.033139  0.242556 -0.054326 -0.025070      1  \n",
      "1630  0.056432  0.011026  0.008531  0.042125 -0.147918 -0.025247      1  \n",
      "\n",
      "[1631 rows x 301 columns]\n",
      "\n",
      "\n",
      "==============\n",
      "logme score (GIN supervised.pth): 3.9831387126006343\n",
      "emb_df: \n",
      "          emb1      emb2      emb3      emb4      emb5      emb6      emb7  \\\n",
      "0    -0.109306  0.026975  0.144055 -0.018794  0.037549  0.003049  0.054056   \n",
      "1    -0.123773  0.061539 -0.037189  0.046046 -0.207685  0.034926  0.121876   \n",
      "2     0.141096  0.308249 -0.004925 -0.062055  0.083926 -0.021454  0.177800   \n",
      "3    -0.092447  0.223640 -0.063829 -0.045427 -0.293054  0.049138 -0.008137   \n",
      "4     0.115564  0.057937 -0.181560  0.234460  0.048131  0.092440  0.067782   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "1626 -0.055354 -0.076412 -0.135973 -0.129253 -0.163806 -0.014976 -0.203393   \n",
      "1627  0.089738  0.164000 -0.104434  0.138175  0.027384  0.014064  0.017805   \n",
      "1628 -0.051994 -0.134461 -0.025678 -0.281383 -0.206925 -0.004779 -0.088676   \n",
      "1629 -0.036588  0.268746  0.120589 -0.189128 -0.134915  0.022022  0.000020   \n",
      "1630  0.028611 -0.160147  0.014808 -0.021268 -0.088051 -0.003266 -0.071580   \n",
      "\n",
      "          emb8      emb9     emb10  ...    emb292    emb293    emb294  \\\n",
      "0    -0.014327 -0.205454 -0.030371  ... -0.080795  0.022156 -0.092458   \n",
      "1    -0.012986 -0.029077  0.008757  ...  0.048660 -0.051925 -0.064342   \n",
      "2     0.046639  0.012898 -0.226528  ...  0.296241 -0.023527  0.162234   \n",
      "3    -0.031451  0.207078 -0.018752  ...  0.223405 -0.149643 -0.022125   \n",
      "4    -0.045703  0.039855  0.074568  ... -0.069917 -0.063802  0.142658   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "1626  0.029238  0.259763  0.034075  ...  0.235216 -0.112416  0.018162   \n",
      "1627  0.016141 -0.134823  0.121302  ... -0.147174 -0.176415 -0.044966   \n",
      "1628  0.002919  0.100805  0.060249  ... -0.060174 -0.099834 -0.052169   \n",
      "1629 -0.079533  0.089986 -0.176976  ... -0.173566 -0.086818  0.097258   \n",
      "1630  0.061780 -0.042499 -0.124031  ...  0.391030 -0.010715 -0.104581   \n",
      "\n",
      "        emb295    emb296    emb297    emb298    emb299    emb300  label  \n",
      "0    -0.079901  0.017925 -0.022426  0.133389 -0.098135 -0.391275      1  \n",
      "1    -0.375008  0.019520 -0.082819  0.231906 -0.114102  0.208744      1  \n",
      "2     0.028958  0.012626 -0.051425  0.177663 -0.048379  0.129985      1  \n",
      "3    -0.303774  0.020329 -0.126647  0.220384  0.119292  0.222148      1  \n",
      "4    -0.100933  0.022558 -0.154368  0.253426  0.136819 -0.206482      1  \n",
      "...        ...       ...       ...       ...       ...       ...    ...  \n",
      "1626 -0.222007  0.014492  0.010279 -0.078890  0.202088  0.220019      1  \n",
      "1627  0.104197  0.015968  0.019625  0.132606  0.325641  0.306807      1  \n",
      "1628 -0.019144  0.016377  0.024919  0.063000  0.060491  0.202242      1  \n",
      "1629  0.021188  0.013450 -0.073449  0.147330 -0.085962  0.142634      1  \n",
      "1630 -0.206350  0.018098 -0.053549  0.196359  0.094236  0.306535      1  \n",
      "\n",
      "[1631 rows x 301 columns]\n",
      "\n",
      "\n",
      "==============\n",
      "logme score (GIN supervised_infomax.pth): 3.9048015813874812\n",
      "emb_df: \n",
      "          emb1      emb2      emb3      emb4      emb5      emb6      emb7  \\\n",
      "0     0.191998 -0.064049 -0.116739  0.046994 -0.033973  0.117178 -0.019639   \n",
      "1    -0.080722 -0.080679  0.027919  0.020533 -0.050606 -0.154942 -0.022649   \n",
      "2     0.043193 -0.117879  0.102459 -0.078738 -0.173783  0.104060 -0.030216   \n",
      "3    -0.125651  0.252041 -0.016143 -0.024959 -0.043136 -0.150652 -0.008667   \n",
      "4    -0.000346  0.090160  0.046422 -0.059993 -0.118659 -0.075911 -0.024074   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "1626 -0.073462  0.168349 -0.100001 -0.033246  0.170729  0.045705 -0.018161   \n",
      "1627  0.206869 -0.293219 -0.098882  0.033391 -0.186220 -0.041448  0.001112   \n",
      "1628  0.064388 -0.028608 -0.085656 -0.018110 -0.028657  0.062794 -0.012044   \n",
      "1629  0.158623  0.148549 -0.034936  0.152476  0.220705 -0.137903  0.004368   \n",
      "1630  0.003257 -0.169705 -0.124157  0.008273  0.092758  0.086454 -0.009367   \n",
      "\n",
      "          emb8      emb9     emb10  ...    emb292    emb293    emb294  \\\n",
      "0    -0.221121 -0.014255 -0.003666  ... -0.000642 -0.028232 -0.057938   \n",
      "1     0.189350 -0.015762 -0.000894  ... -0.006836 -0.029507 -0.067649   \n",
      "2     0.150640 -0.032769 -0.000679  ... -0.024111  0.002887  0.125609   \n",
      "3    -0.003388  0.050739 -0.001665  ...  0.003550 -0.035003 -0.072075   \n",
      "4     0.026159  0.043569  0.000010  ...  0.016046 -0.029881 -0.105250   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "1626  0.101702 -0.070972 -0.001428  ...  0.003428 -0.023884 -0.117733   \n",
      "1627  0.106268  0.148866 -0.006524  ...  0.018949 -0.077309 -0.344757   \n",
      "1628  0.072997 -0.014626 -0.003126  ...  0.013951 -0.038035 -0.003392   \n",
      "1629 -0.035570  0.061323 -0.003969  ... -0.005269 -0.009305  0.095928   \n",
      "1630  0.017095 -0.042023 -0.003787  ...  0.005363 -0.018604  0.023921   \n",
      "\n",
      "        emb295    emb296    emb297    emb298    emb299    emb300  label  \n",
      "0    -0.061422  0.049913 -0.002016 -0.055694  0.064503  0.042552      1  \n",
      "1    -0.500705 -0.048177  0.004055 -0.091411  0.066462  0.041825      1  \n",
      "2    -0.347501  0.104612  0.008131 -0.000819 -0.022134  0.048271      1  \n",
      "3    -0.199246 -0.040050  0.001687 -0.117823  0.034103  0.102362      1  \n",
      "4    -0.220897  0.007098  0.004599 -0.094198  0.051291 -0.003465      1  \n",
      "...        ...       ...       ...       ...       ...       ...    ...  \n",
      "1626 -0.136909 -0.115281  0.000761 -0.013600  0.008555 -0.013329      1  \n",
      "1627  0.011398  0.044437 -0.010820  0.100201 -0.032079 -0.201765      1  \n",
      "1628 -0.025984 -0.054815 -0.001173 -0.019180  0.006517  0.036066      1  \n",
      "1629 -0.127344 -0.169471 -0.002898  0.163809  0.162195  0.178764      1  \n",
      "1630 -0.147082 -0.001464  0.004375 -0.105898 -0.114983  0.044680      1  \n",
      "\n",
      "[1631 rows x 301 columns]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============\n",
      "logme score (GIN supervised_edgepred.pth): 3.970161485661006\n",
      "emb_df: \n",
      "          emb1      emb2      emb3      emb4      emb5      emb6      emb7  \\\n",
      "0    -0.005460  0.051137 -0.114106 -0.029731 -0.142455 -0.043726  0.147545   \n",
      "1     0.033536 -0.056563  0.036576  0.175933  0.134396  0.189540 -0.235593   \n",
      "2     0.145353  0.026588  0.356795 -0.211925  0.133437 -0.174142  0.155910   \n",
      "3     0.074380 -0.044443  0.029541  0.112440 -0.004850  0.177624 -0.220913   \n",
      "4     0.181920 -0.129171 -0.027031  0.109574 -0.039084  0.092776 -0.116252   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "1626 -0.062165 -0.111285  0.019402 -0.068983 -0.020654 -0.092148 -0.132049   \n",
      "1627  0.017005  0.162836 -0.020215 -0.059957 -0.123588 -0.136370  0.006118   \n",
      "1628 -0.130397  0.098640 -0.019299 -0.017283  0.127086 -0.068968  0.087288   \n",
      "1629  0.064985  0.027036  0.053525 -0.010578 -0.052231 -0.007877  0.135360   \n",
      "1630  0.209731  0.017934 -0.033849 -0.182742 -0.210122  0.126560 -0.138507   \n",
      "\n",
      "          emb8      emb9     emb10  ...    emb292    emb293    emb294  \\\n",
      "0    -0.002725 -0.033225  0.063732  ... -0.015921  0.023579 -0.092539   \n",
      "1    -0.123073  0.028745 -0.025497  ...  0.030821  0.115594 -0.162112   \n",
      "2     0.026206 -0.187078 -0.132512  ... -0.027948  0.134913 -0.046673   \n",
      "3    -0.162864  0.037121 -0.173668  ...  0.009805 -0.053253 -0.109941   \n",
      "4    -0.038932  0.038050 -0.035311  ...  0.017599 -0.093972 -0.170444   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "1626 -0.169590 -0.122028  0.120785  ... -0.010746  0.013012  0.217536   \n",
      "1627  0.070792  0.118196 -0.089530  ...  0.009532  0.117400  0.209849   \n",
      "1628  0.123672 -0.103156  0.002332  ... -0.022389 -0.051534  0.050185   \n",
      "1629  0.166965 -0.058526 -0.004265  ... -0.025067 -0.040564  0.037534   \n",
      "1630 -0.105905  0.015551 -0.040348  ... -0.009209  0.174873 -0.025266   \n",
      "\n",
      "        emb295    emb296    emb297    emb298    emb299    emb300  label  \n",
      "0    -0.142574 -0.018901  0.070106  0.003823 -0.048418 -0.005926      1  \n",
      "1    -0.162284 -0.029262 -0.003771 -0.062431 -0.219849  0.165700      1  \n",
      "2    -0.055740 -0.010988  0.020360  0.048176 -0.056471  0.207465      1  \n",
      "3    -0.061642 -0.028102  0.075296 -0.105591 -0.299435  0.011499      1  \n",
      "4    -0.191881 -0.034133  0.062344 -0.064522 -0.072520  0.109797      1  \n",
      "...        ...       ...       ...       ...       ...       ...    ...  \n",
      "1626 -0.010095 -0.022178  0.055461 -0.102274  0.079285 -0.172534      1  \n",
      "1627 -0.055981 -0.016721  0.074855  0.002792  0.254813  0.062543      1  \n",
      "1628  0.024604 -0.021837  0.047494 -0.107094  0.085930 -0.007207      1  \n",
      "1629  0.036291 -0.019698 -0.065197 -0.101500  0.004703  0.034508      1  \n",
      "1630 -0.064813 -0.012954  0.056954  0.148119  0.011343  0.211372      1  \n",
      "\n",
      "[1631 rows x 301 columns]\n",
      "\n",
      "\n",
      "==============\n",
      "logme score (GIN supervised_masking.pth): 3.9627838387752337\n",
      "emb_df: \n",
      "          emb1      emb2      emb3      emb4      emb5      emb6      emb7  \\\n",
      "0     0.000824  0.171082  0.018847  0.113691 -0.176333  0.143199  0.007195   \n",
      "1    -0.004644 -0.205525  0.096942  0.249130 -0.030214  0.263173  0.022912   \n",
      "2    -0.043254 -0.039752 -0.066868 -0.032429 -0.241200  0.084242 -0.023392   \n",
      "3    -0.004363 -0.206500  0.009979  0.005574 -0.052760  0.047711  0.018011   \n",
      "4     0.059399 -0.047559  0.085573  0.153698  0.134737  0.099420  0.059221   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "1626  0.005610 -0.018763  0.187947  0.032897 -0.175292  0.045307  0.013426   \n",
      "1627  0.005252 -0.103012  0.012903 -0.140464 -0.008857  0.081753  0.028195   \n",
      "1628  0.004041 -0.046565  0.050531  0.140908  0.053520  0.067464  0.003140   \n",
      "1629 -0.043028 -0.014708 -0.136210 -0.306549 -0.249404 -0.282627 -0.017116   \n",
      "1630  0.010971  0.020905  0.114980  0.028400 -0.214816  0.076810  0.030607   \n",
      "\n",
      "          emb8      emb9     emb10  ...    emb292    emb293    emb294  \\\n",
      "0    -0.013865 -0.105762 -0.009721  ... -0.004086  0.033476  0.020375   \n",
      "1    -0.142746  0.064652  0.009795  ... -0.005236  0.029684  0.069953   \n",
      "2     0.205746  0.002907 -0.033210  ... -0.002073  0.046493  0.020846   \n",
      "3    -0.120675 -0.095415  0.009273  ... -0.005712  0.037507  0.059162   \n",
      "4    -0.017932  0.047147  0.015642  ... -0.006622 -0.002600  0.032375   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "1626  0.072797  0.008504 -0.010689  ... -0.003769  0.036893  0.017523   \n",
      "1627 -0.049338 -0.063672 -0.004830  ... -0.004262  0.017506  0.063400   \n",
      "1628  0.063526 -0.045610 -0.005283  ... -0.005193  0.036696 -0.000493   \n",
      "1629  0.254401  0.183268 -0.006929  ... -0.003406  0.060471  0.049676   \n",
      "1630  0.089958 -0.077953 -0.001975  ... -0.004357  0.029839  0.024964   \n",
      "\n",
      "        emb295    emb296    emb297    emb298    emb299    emb300  label  \n",
      "0    -0.010525  0.001720 -0.110601 -0.027548 -0.020588  0.055638      1  \n",
      "1    -0.009630  0.005868  0.056542 -0.005886  0.001713 -0.250014      1  \n",
      "2    -0.010545 -0.152520 -0.060676 -0.054954 -0.027366  0.078744      1  \n",
      "3    -0.010064  0.084531  0.097109 -0.082896 -0.027742 -0.058619      1  \n",
      "4    -0.007344  0.083644 -0.035466 -0.027070  0.103695 -0.108191      1  \n",
      "...        ...       ...       ...       ...       ...       ...    ...  \n",
      "1626 -0.010493 -0.004179 -0.003951 -0.031623 -0.017937  0.105980      1  \n",
      "1627 -0.010653  0.096363  0.005645 -0.055255 -0.131929  0.132470      1  \n",
      "1628 -0.009986  0.077027 -0.017333  0.033037 -0.102384 -0.124321      1  \n",
      "1629 -0.011209  0.051938 -0.065190  0.166391  0.159966 -0.073262      1  \n",
      "1630 -0.010308 -0.048768 -0.070209 -0.033793 -0.057373 -0.059358      1  \n",
      "\n",
      "[1631 rows x 301 columns]\n",
      "\n",
      "\n",
      "==============\n",
      "logme score (GIN supervised_contextpred.pth): 3.957659679932999\n",
      "emb_df: \n",
      "          emb1      emb2      emb3      emb4      emb5      emb6      emb7  \\\n",
      "0    -0.000087 -0.002215  0.000219 -1.117616 -0.001201  0.000990 -0.190259   \n",
      "1    -0.002966 -0.006658 -0.001822 -1.556792 -0.001788  0.002202  0.024980   \n",
      "2    -0.000656 -0.003309 -0.000491 -0.241810 -0.001006  0.000025 -0.277406   \n",
      "3    -0.001692 -0.002955 -0.001532 -0.753289 -0.001696  0.001543 -0.132994   \n",
      "4    -0.001137 -0.000483  0.000705 -0.281938 -0.000569 -0.001191  0.087371   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "1626 -0.001616 -0.003095 -0.000879 -1.109064 -0.002527  0.000747 -0.311474   \n",
      "1627 -0.000309 -0.001394  0.000053 -0.327151 -0.001027 -0.000509 -0.314331   \n",
      "1628 -0.000519 -0.003734 -0.000613 -1.772292 -0.001177  0.000626 -0.193273   \n",
      "1629 -0.001857 -0.004978 -0.000934 -0.302998 -0.001659  0.001972 -0.188480   \n",
      "1630 -0.000412 -0.001442 -0.000934 -0.180453 -0.001730  0.001908 -0.289632   \n",
      "\n",
      "          emb8      emb9     emb10  ...    emb292    emb293    emb294  \\\n",
      "0    -1.267744  0.001198 -0.001750  ... -0.017830 -0.908909  0.003436   \n",
      "1    -0.998696  0.000863 -0.002154  ... -0.008353 -1.620070  0.002529   \n",
      "2    -0.990562  0.002525 -0.001843  ... -0.004904 -0.698251  0.002733   \n",
      "3    -0.585159  0.001457 -0.001762  ... -0.010418 -0.672801  0.004794   \n",
      "4    -0.229242  0.002509 -0.000662  ...  0.001450 -0.646833  0.000523   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "1626 -0.527182  0.002188 -0.000880  ... -0.014563 -3.193035  0.002766   \n",
      "1627 -0.820273  0.002791 -0.000647  ... -0.008035 -0.878781  0.000601   \n",
      "1628 -0.519648  0.001843 -0.001159  ... -0.006080 -3.776071  0.001985   \n",
      "1629 -0.223861  0.001558 -0.001353  ... -0.019975 -0.524582  0.004152   \n",
      "1630 -0.621222  0.001480 -0.001569  ... -0.013283 -1.201333  0.002710   \n",
      "\n",
      "        emb295    emb296    emb297    emb298    emb299    emb300  label  \n",
      "0    -0.327803 -0.003596  0.000422 -1.584143 -0.000309 -1.081355      1  \n",
      "1    -0.203583 -0.003671  0.001096 -0.476106  0.000243 -0.619354      1  \n",
      "2     0.301109 -0.003485  0.001347 -0.448092 -0.000161 -0.643368      1  \n",
      "3     1.532645 -0.003652  0.000791 -0.543264  0.000476 -0.207365      1  \n",
      "4    -0.698916 -0.003464 -0.000363 -0.680009 -0.002179 -9.952042      1  \n",
      "...        ...       ...       ...       ...       ...       ...    ...  \n",
      "1626 -0.160331 -0.003563 -0.000169 -4.234638 -0.000297 -1.044845      1  \n",
      "1627 -0.285001 -0.003487 -0.001053 -0.615641 -0.001956 -0.540979      1  \n",
      "1628 -0.070108 -0.003469  0.000424 -1.228648 -0.000237 -0.046761      1  \n",
      "1629 -0.104672 -0.003636  0.003548 -0.129416  0.000415 -0.863676      1  \n",
      "1630 -0.018785 -0.003427  0.000002 -0.179900  0.000274 -0.719474      1  \n",
      "\n",
      "[1631 rows x 301 columns]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============\n",
      "logme score (GIN infomax.pth): 3.9504379329602375\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"bbbp\"\n",
    "num_tasks = 1\n",
    "batch_size = 32 # default\n",
    "num_workers = 4 # default\n",
    "save_results_to = '/mnt/sdc/course-projects/GRL-course-project/results'\n",
    "\n",
    "dataset = MoleculeDataset(\"dataset/\" + dataset_name, dataset=dataset_name)\n",
    "print(dataset)\n",
    "\n",
    "# scaffold split:\n",
    "smiles_list = pd.read_csv('dataset/' + dataset_name + '/processed/smiles.csv', header=None)[0].tolist()\n",
    "train_dataset, valid_dataset, test_dataset = scaffold_split(dataset, smiles_list, null_value=0, frac_train=0.8,frac_valid=0.1, frac_test=0.1)\n",
    "print(train_dataset[0])\n",
    "\n",
    "## set shuffle to FALSE so we can compare the extracted features using different pre-trained networks \n",
    "bbbp_train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers = num_workers)\n",
    "# bbbp_val_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers = num_workers)\n",
    "# bbbp_test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers = num_workers)\n",
    "\n",
    "\n",
    "from LogME import LogME\n",
    "\n",
    "## using GIN supervised model ('./model_gin/supervised.pth') ## \n",
    "\n",
    "bbbp_graph_features, bbbp_graph_labels = get_graph_features_labels(bbbp_train_loader, gin_supervised_model, seed)\n",
    "\n",
    "create_dataframe_save_to_csv(bbbp_graph_features, bbbp_graph_labels, dataset_name, \n",
    "                             model_name='gin_supervised', \n",
    "                             save_path=save_results_to)\n",
    "# assert(False)\n",
    "logme = LogME(regression=False)\n",
    "# f has shape of [N, D], y has shape [N]\n",
    "score = logme.fit(np.array(bbbp_graph_features), np.array(bbbp_graph_labels))\n",
    "print(\"\\n==============\")\n",
    "print(\"logme score (GIN supervised.pth): {}\".format(score))\n",
    "\n",
    "#############################\n",
    "\n",
    "## using GIN supervised infomax model (supervised_infomax.pth) ## \n",
    "\n",
    "bbbp_graph_features, bbbp_graph_labels = get_graph_features_labels(bbbp_train_loader, gin_supervised_infomax_model, seed)\n",
    "\n",
    "create_dataframe_save_to_csv(bbbp_graph_features, bbbp_graph_labels, dataset_name, \n",
    "                             model_name='gin_supervised_infomax', \n",
    "                             save_path=save_results_to)\n",
    "\n",
    "logme = LogME(regression=False)\n",
    "# f has shape of [N, D], y has shape [N]\n",
    "score = logme.fit(np.array(bbbp_graph_features), np.array(bbbp_graph_labels))\n",
    "print(\"\\n==============\")\n",
    "print(\"logme score (GIN supervised_infomax.pth): {}\".format(score))\n",
    "\n",
    "#############################\n",
    "\n",
    "## using GIN supervised edgepred model (supervised_edgepred.pth) ## \n",
    "\n",
    "bbbp_graph_features, bbbp_graph_labels = get_graph_features_labels(bbbp_train_loader, gin_supervised_edgepred_model, seed)\n",
    "\n",
    "create_dataframe_save_to_csv(bbbp_graph_features, bbbp_graph_labels, dataset_name, \n",
    "                             model_name='gin_supervised_edgepred', \n",
    "                             save_path=save_results_to)\n",
    "\n",
    "logme = LogME(regression=False)\n",
    "# f has shape of [N, D], y has shape [N]\n",
    "score = logme.fit(np.array(bbbp_graph_features), np.array(bbbp_graph_labels))\n",
    "print(\"\\n==============\")\n",
    "print(\"logme score (GIN supervised_edgepred.pth): {}\".format(score))\n",
    "\n",
    "#############################\n",
    "\n",
    "## using GIN supervised masking model (supervised_masking.pth) ## \n",
    "\n",
    "bbbp_graph_features, bbbp_graph_labels = get_graph_features_labels(bbbp_train_loader, gin_supervised_masking_model, seed)\n",
    "\n",
    "create_dataframe_save_to_csv(bbbp_graph_features, bbbp_graph_labels, dataset_name, \n",
    "                             model_name='gin_supervised_masking', \n",
    "                             save_path=save_results_to)\n",
    "\n",
    "logme = LogME(regression=False)\n",
    "# f has shape of [N, D], y has shape [N]\n",
    "score = logme.fit(np.array(bbbp_graph_features), np.array(bbbp_graph_labels))\n",
    "print(\"\\n==============\")\n",
    "print(\"logme score (GIN supervised_masking.pth): {}\".format(score))\n",
    "\n",
    "#############################\n",
    "\n",
    "## using GIN supervised contextpred model (supervised_contextpred.pth) ## \n",
    "\n",
    "bbbp_graph_features, bbbp_graph_labels = get_graph_features_labels(bbbp_train_loader, gin_supervised_contextpred_model, seed)\n",
    "\n",
    "create_dataframe_save_to_csv(bbbp_graph_features, bbbp_graph_labels, dataset_name, \n",
    "                             model_name='gin_supervised_contextpred', \n",
    "                             save_path=save_results_to)\n",
    "\n",
    "logme = LogME(regression=False)\n",
    "# f has shape of [N, D], y has shape [N]\n",
    "score = logme.fit(np.array(bbbp_graph_features), np.array(bbbp_graph_labels))\n",
    "print(\"\\n==============\")\n",
    "print(\"logme score (GIN supervised_contextpred.pth): {}\".format(score))\n",
    "\n",
    "#############################\n",
    "\n",
    "## using GIN infomax model (infomax.pth) ## \n",
    "\n",
    "bbbp_graph_features, bbbp_graph_labels = get_graph_features_labels(bbbp_train_loader, gin_infomax_model, seed)\n",
    "\n",
    "create_dataframe_save_to_csv(bbbp_graph_features, bbbp_graph_labels, dataset_name, \n",
    "                             model_name='gin_infomax', \n",
    "                             save_path=save_results_to)\n",
    "\n",
    "logme = LogME(regression=False)\n",
    "# f has shape of [N, D], y has shape [N]\n",
    "score = logme.fit(np.array(bbbp_graph_features), np.array(bbbp_graph_labels))\n",
    "print(\"\\n==============\")\n",
    "print(\"logme score (GIN infomax.pth): {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c46077b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0\n",
      "batch: Batch(batch=[500], edge_attr=[998, 2], edge_index=[2, 998], id=[32], x=[500, 2], y=[32])\n",
      "batch.batch: tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  4,  4,  4,  4,\n",
      "         4,  4,  4,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
      "         5,  5,  5,  5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "         7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,\n",
      "         8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
      "         9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "        10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "        11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13,\n",
      "        13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
      "        14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "        17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20,\n",
      "        20, 20, 20, 20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
      "        21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23,\n",
      "        23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 24,\n",
      "        24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25,\n",
      "        25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26, 26, 26,\n",
      "        26, 26, 26, 26, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
      "        27, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31,\n",
      "        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31])\n",
      "batch.x: tensor([[ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 6,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [16,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [16,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  2],\n",
      "        [ 6,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [15,  0],\n",
      "        [ 7,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  1],\n",
      "        [ 7,  0],\n",
      "        [ 5,  1],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 6,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [16,  0],\n",
      "        [16,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 6,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 6,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 6,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 6,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 6,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 6,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 6,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  1],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 6,  0],\n",
      "        [ 6,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 7,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 6,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [16,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [16,  0],\n",
      "        [ 6,  0],\n",
      "        [ 6,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [15,  0],\n",
      "        [ 7,  0],\n",
      "        [ 7,  0],\n",
      "        [ 7,  0],\n",
      "        [ 6,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 6,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 6,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [16,  0],\n",
      "        [16,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 6,  0],\n",
      "        [ 7,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 6,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 6,  0],\n",
      "        [15,  0],\n",
      "        [ 7,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [16,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [16,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [16,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 6,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 6,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 6,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 6,  0],\n",
      "        [ 5,  1],\n",
      "        [ 5,  0],\n",
      "        [ 5,  2],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 6,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  2],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [16,  0],\n",
      "        [ 5,  0],\n",
      "        [16,  0],\n",
      "        [ 6,  0],\n",
      "        [ 5,  0],\n",
      "        [ 6,  0],\n",
      "        [ 6,  0],\n",
      "        [ 6,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [16,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [16,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  2],\n",
      "        [ 6,  0],\n",
      "        [ 5,  2],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 6,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 6,  0],\n",
      "        [ 5,  1],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 6,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 6,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 6,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [16,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [16,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  2],\n",
      "        [ 6,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 5,  0],\n",
      "        [ 5,  0],\n",
      "        [ 7,  0],\n",
      "        [ 7,  0]])\n",
      "batch.edge_attr: tensor([[1, 0],\n",
      "        [1, 0],\n",
      "        [0, 0],\n",
      "        ...,\n",
      "        [1, 0],\n",
      "        [3, 0],\n",
      "        [3, 0]])\n"
     ]
    }
   ],
   "source": [
    "## test ## \n",
    "for step, batch in enumerate(bbbp_train_loader):\n",
    "    print(\"step: {}\".format(step))\n",
    "    print(\"batch: {}\".format(batch))\n",
    "    print(\"batch.batch: {}\".format(batch.batch))\n",
    "    print(\"batch.x: {}\".format(batch.x))\n",
    "    print(\"batch.edge_attr: {}\".format(batch.edge_attr))\n",
    "    break\n",
    "## end of test ## \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44180799",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-th feature | mean: -0.23916535079479218 +- 0.8351194262504578\n",
      "1-th feature | mean: -0.19498318433761597 +- 0.8193398118019104\n",
      "2-th feature | mean: -0.2217152863740921 +- 0.8073065876960754\n",
      "3-th feature | mean: -0.2068459838628769 +- 0.9316534399986267\n",
      "4-th feature | mean: -0.09914200752973557 +- 0.6641184687614441\n",
      "5-th feature | mean: -0.22375477850437164 +- 0.7601909637451172\n",
      "6-th feature | mean: -0.24704651534557343 +- 0.7580361366271973\n",
      "7-th feature | mean: -0.17532235383987427 +- 0.9688711166381836\n",
      "8-th feature | mean: -0.2324436455965042 +- 0.9376943707466125\n",
      "9-th feature | mean: -0.2381032258272171 +- 0.866077721118927\n",
      "10-th feature | mean: -0.23781614005565643 +- 0.9739698171615601\n",
      "11-th feature | mean: -0.1704014390707016 +- 0.6410852670669556\n",
      "12-th feature | mean: -0.24489985406398773 +- 0.9539623260498047\n",
      "13-th feature | mean: -0.1664772629737854 +- 1.006030559539795\n",
      "14-th feature | mean: -0.24989280104637146 +- 0.8938428163528442\n",
      "15-th feature | mean: -0.23559331893920898 +- 0.846644937992096\n",
      "16-th feature | mean: -0.20322275161743164 +- 0.8412976264953613\n",
      "17-th feature | mean: -0.16888906061649323 +- 0.8324511051177979\n",
      "18-th feature | mean: -0.27641692757606506 +- 0.8858612775802612\n",
      "19-th feature | mean: -0.168878436088562 +- 0.9533649682998657\n",
      "20-th feature | mean: -0.20846541225910187 +- 1.1174973249435425\n",
      "21-th feature | mean: -0.22301270067691803 +- 0.9546970129013062\n",
      "22-th feature | mean: -0.2133089005947113 +- 0.8568872213363647\n",
      "23-th feature | mean: -0.2574884593486786 +- 0.8030961155891418\n",
      "24-th feature | mean: -0.28720834851264954 +- 1.0387578010559082\n",
      "25-th feature | mean: -0.2515512704849243 +- 1.017362117767334\n",
      "26-th feature | mean: -0.20486202836036682 +- 0.8948960304260254\n",
      "27-th feature | mean: -0.17358553409576416 +- 0.8921122550964355\n",
      "28-th feature | mean: -0.2472047358751297 +- 0.9883662462234497\n",
      "29-th feature | mean: -0.2060355693101883 +- 0.9887517690658569\n",
      "30-th feature | mean: -0.234099343419075 +- 0.9688902497291565\n",
      "31-th feature | mean: -0.25546717643737793 +- 0.9615544676780701\n",
      "32-th feature | mean: -0.23800526559352875 +- 0.9847215414047241\n",
      "33-th feature | mean: -0.2534348666667938 +- 0.9553108811378479\n",
      "34-th feature | mean: -0.1439330279827118 +- 0.9471707344055176\n",
      "35-th feature | mean: -0.15816165506839752 +- 0.8486934900283813\n",
      "36-th feature | mean: -0.23917989432811737 +- 0.8447883725166321\n",
      "37-th feature | mean: -0.1665467470884323 +- 0.8139142990112305\n",
      "38-th feature | mean: -0.19995835423469543 +- 0.963792622089386\n",
      "39-th feature | mean: -0.030913865193724632 +- 0.36097997426986694\n",
      "40-th feature | mean: -0.2273121178150177 +- 0.8225537538528442\n",
      "41-th feature | mean: -0.16613832116127014 +- 0.8489693999290466\n",
      "42-th feature | mean: -0.09914200752973557 +- 0.6641184687614441\n",
      "43-th feature | mean: -0.2409171313047409 +- 0.9016878604888916\n",
      "44-th feature | mean: -0.2646886110305786 +- 0.8810880184173584\n",
      "45-th feature | mean: -0.24922241270542145 +- 0.9624355435371399\n",
      "46-th feature | mean: -0.25434502959251404 +- 0.9026356935501099\n",
      "47-th feature | mean: -0.032401759177446365 +- 0.28487008810043335\n",
      "48-th feature | mean: -0.08616757392883301 +- 0.5572835803031921\n",
      "49-th feature | mean: -0.18461835384368896 +- 1.027925968170166\n",
      "50-th feature | mean: -0.07643139362335205 +- 0.41641107201576233\n",
      "51-th feature | mean: -0.0839918926358223 +- 0.6200733184814453\n"
     ]
    }
   ],
   "source": [
    "## check to see if the data is loaded in the same order every time # \n",
    "for i, feature in enumerate(bbbp_graph_features):\n",
    "#     print(\"{}-th feature: \\n{}\".format(i, feature))\n",
    "    print(\"{}-th feature | mean: {} +- {}\".format(i, np.mean(feature), np.std(feature)))\n",
    "    if i > 50: \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19236960",
   "metadata": {},
   "source": [
    "## Extract features of BACE training set using pre-trained GNN & obtain LogME score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94ba976c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoleculeDataset(1513)\n",
      "Data(edge_attr=[66, 2], edge_index=[2, 66], fold=[1], id=[1], x=[31, 2], y=[1])\n",
      "emb_df: \n",
      "          emb1      emb2      emb3      emb4      emb5      emb6      emb7  \\\n",
      "0    -0.028116  0.005691 -0.137788  0.029680  0.016036 -0.003200  0.048668   \n",
      "1     0.060740  0.016581 -0.228648 -0.045908 -0.003714  0.039868 -0.005419   \n",
      "2     0.069248  0.010116 -0.066165 -0.031711  0.027497 -0.053612  0.022585   \n",
      "3    -0.039441  0.009469  0.042803  0.003924  0.012666 -0.019713 -0.072263   \n",
      "4     0.049737  0.019437 -0.179696 -0.120685 -0.054918 -0.076108  0.018942   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "1205  0.038832  0.019011 -0.189996 -0.140201 -0.065311 -0.066743 -0.019755   \n",
      "1206 -0.029755  0.013644 -0.033361  0.014733  0.068999 -0.040916 -0.023484   \n",
      "1207  0.031380  0.009848 -0.047594  0.032608  0.020400 -0.009646  0.104117   \n",
      "1208  0.028505  0.012025 -0.130497 -0.025724  0.051418 -0.084800 -0.040346   \n",
      "1209  0.066476  0.011486 -0.054855 -0.032764  0.021737 -0.046622 -0.022343   \n",
      "\n",
      "          emb8      emb9     emb10  ...    emb292    emb293    emb294  \\\n",
      "0    -0.009361 -0.015647 -0.069469  ... -0.010155  0.011615 -0.043586   \n",
      "1    -0.019419  0.024565 -0.018187  ... -0.000377 -0.019600 -0.020432   \n",
      "2    -0.026778 -0.108778  0.094168  ... -0.004585  0.047872 -0.017619   \n",
      "3     0.130793 -0.116495 -0.003532  ... -0.021350  0.039170 -0.029081   \n",
      "4    -0.005468 -0.049055 -0.062397  ...  0.004651 -0.065392 -0.028106   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "1205  0.004221 -0.102985 -0.076603  ...  0.004060 -0.057654 -0.056476   \n",
      "1206 -0.024382  0.011243 -0.033669  ... -0.000890  0.023430  0.007572   \n",
      "1207  0.055834  0.025455 -0.037763  ... -0.001952  0.046664  0.049110   \n",
      "1208 -0.061197  0.035928 -0.083760  ... -0.000324  0.001196  0.030976   \n",
      "1209  0.008406 -0.095658 -0.034520  ... -0.001878  0.074236 -0.037010   \n",
      "\n",
      "        emb295    emb296    emb297    emb298    emb299    emb300  label  \n",
      "0    -0.017337 -0.021150  0.026314 -0.043637 -0.034137 -0.024655     -1  \n",
      "1    -0.032078 -0.073176  0.038408  0.029231  0.051132 -0.024761      1  \n",
      "2    -0.026676 -0.054443 -0.056225  0.057045  0.043631 -0.025345     -1  \n",
      "3     0.078116 -0.033222  0.026531  0.076517 -0.002709 -0.024722      1  \n",
      "4    -0.059868  0.034180  0.012342 -0.039965  0.057727 -0.025495     -1  \n",
      "...        ...       ...       ...       ...       ...       ...    ...  \n",
      "1205 -0.063212  0.037692  0.005026 -0.021436  0.012550 -0.025467      1  \n",
      "1206 -0.012729 -0.092698 -0.000351  0.071313 -0.130066 -0.025278      1  \n",
      "1207 -0.029908 -0.004664  0.016887  0.013091  0.049205 -0.024463     -1  \n",
      "1208  0.028846  0.021260 -0.094364  0.062930  0.008506 -0.025294      1  \n",
      "1209  0.004609 -0.005729  0.030743  0.090111 -0.000980 -0.024966     -1  \n",
      "\n",
      "[1210 rows x 301 columns]\n",
      "\n",
      "\n",
      "==============\n",
      "logme score (GIN supervised.pth): 3.814949522466228\n",
      "emb_df: \n",
      "          emb1      emb2      emb3      emb4      emb5      emb6      emb7  \\\n",
      "0    -0.089315 -0.347077  0.163486  0.063849 -0.262107 -0.008528 -0.075204   \n",
      "1     0.063726  0.109320 -0.050776 -0.114106 -0.137143 -0.025371 -0.038267   \n",
      "2     0.048124 -0.251532 -0.003734 -0.055539 -0.006525  0.002886 -0.069199   \n",
      "3    -0.083163 -0.309727 -0.074801  0.050859  0.078568 -0.036909  0.128004   \n",
      "4    -0.071514 -0.000651 -0.008315  0.089956  0.356516 -0.009952 -0.004291   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "1205 -0.014540 -0.046336  0.116849 -0.038677  0.387307 -0.020673 -0.115150   \n",
      "1206  0.055365 -0.159982  0.058044  0.042791 -0.069220 -0.019849 -0.098549   \n",
      "1207 -0.004474  0.237302  0.068053 -0.036371  0.038982 -0.000602 -0.101267   \n",
      "1208 -0.112306 -0.058157  0.055079 -0.094567 -0.008741 -0.019221  0.106223   \n",
      "1209 -0.065220 -0.131729  0.124647 -0.062053  0.087639 -0.006657 -0.049510   \n",
      "\n",
      "          emb8      emb9     emb10  ...    emb292    emb293    emb294  \\\n",
      "0     0.012683 -0.276000 -0.007128  ...  0.039401 -0.070828 -0.005839   \n",
      "1     0.037050 -0.044885 -0.121660  ...  0.000528 -0.031211  0.104081   \n",
      "2     0.056537  0.145138 -0.045193  ... -0.023234 -0.017946  0.007658   \n",
      "3    -0.037140 -0.150176 -0.046369  ... -0.044738 -0.160829 -0.047170   \n",
      "4     0.033312  0.206410 -0.035003  ... -0.035332  0.027472 -0.010591   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "1205  0.030720  0.232496  0.081722  ... -0.058151  0.019083 -0.023096   \n",
      "1206 -0.050725 -0.032697  0.200108  ... -0.029663 -0.090197  0.066804   \n",
      "1207 -0.045659 -0.040260  0.166592  ...  0.025957 -0.009923  0.029119   \n",
      "1208 -0.006129 -0.040657 -0.185373  ... -0.003421 -0.017832 -0.178647   \n",
      "1209 -0.019939 -0.193192 -0.195510  ... -0.414074 -0.180125 -0.042302   \n",
      "\n",
      "        emb295    emb296    emb297    emb298    emb299    emb300  label  \n",
      "0     0.006634  0.014785  0.003463  0.027479 -0.043422  0.141815     -1  \n",
      "1    -0.164144  0.011920  0.125204 -0.018783 -0.132431  0.001907      1  \n",
      "2    -0.202752  0.017231  0.023874  0.162598  0.153141 -0.097143     -1  \n",
      "3    -0.148735  0.011617 -0.051396  0.116568 -0.033461  0.164021      1  \n",
      "4     0.034346  0.015240  0.058156 -0.047955 -0.025101 -0.110047     -1  \n",
      "...        ...       ...       ...       ...       ...       ...    ...  \n",
      "1205 -0.001832  0.015185  0.100745 -0.058291  0.016466  0.060861      1  \n",
      "1206 -0.263339  0.015108  0.030003  0.130886  0.019653 -0.290767      1  \n",
      "1207 -0.005029  0.016999 -0.001243 -0.284207 -0.014532  0.064275     -1  \n",
      "1208  0.091973  0.015667  0.042805  0.017890  0.325672 -0.190449      1  \n",
      "1209 -0.068412  0.013105 -0.048383 -0.018692  0.376285 -0.059030     -1  \n",
      "\n",
      "[1210 rows x 301 columns]\n",
      "\n",
      "\n",
      "==============\n",
      "logme score (GIN supervised_infomax.pth): 3.7664541533452516\n",
      "emb_df: \n",
      "          emb1      emb2      emb3      emb4      emb5      emb6      emb7  \\\n",
      "0    -0.061251 -0.016543 -0.051094 -0.057883  0.052493 -0.178966 -0.013484   \n",
      "1    -0.141531  0.076137 -0.050722 -0.120655 -0.009346 -0.063064 -0.006525   \n",
      "2    -0.062841 -0.064925  0.025600 -0.102735 -0.073917 -0.161443 -0.014954   \n",
      "3    -0.142770 -0.061186  0.018299 -0.055421  0.006874 -0.102253 -0.008411   \n",
      "4    -0.012804 -0.121383 -0.013708 -0.079420  0.001500 -0.146589 -0.005923   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "1205  0.052865 -0.107197  0.005085 -0.081911  0.002239 -0.118772 -0.007743   \n",
      "1206  0.047511 -0.001141  0.062329 -0.039200 -0.035146 -0.090816 -0.014242   \n",
      "1207  0.118901 -0.077345  0.026631  0.026899 -0.132179  0.190121 -0.032362   \n",
      "1208  0.015131 -0.103123 -0.050047  0.034765  0.032710 -0.115973 -0.012144   \n",
      "1209 -0.061411  0.119695  0.070266  0.131256  0.079176  0.026834 -0.005505   \n",
      "\n",
      "          emb8      emb9     emb10  ...    emb292    emb293    emb294  \\\n",
      "0    -0.017177 -0.058953 -0.003823  ... -0.003980 -0.031105 -0.013247   \n",
      "1    -0.045160 -0.156248 -0.004724  ...  0.003388 -0.036571  0.169398   \n",
      "2     0.001409  0.047724 -0.002759  ...  0.003928 -0.039297  0.085962   \n",
      "3     0.126136 -0.114040 -0.006412  ...  0.007560 -0.029911 -0.164635   \n",
      "4     0.052862  0.078816 -0.004689  ... -0.008129 -0.042689 -0.067285   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "1205  0.130549  0.059319 -0.005097  ... -0.005596 -0.043695 -0.027733   \n",
      "1206 -0.044309 -0.034513 -0.003939  ...  0.002315 -0.032555  0.123989   \n",
      "1207 -0.331823  0.110611 -0.003413  ...  0.004358 -0.030530 -0.010225   \n",
      "1208  0.178633  0.132175 -0.004282  ...  0.000609 -0.035446  0.125260   \n",
      "1209  0.072430 -0.126447 -0.003969  ...  0.014731 -0.042204 -0.097019   \n",
      "\n",
      "        emb295    emb296    emb297    emb298    emb299    emb300  label  \n",
      "0    -0.182815 -0.093450 -0.001596 -0.026985  0.034272 -0.021588     -1  \n",
      "1     0.086470 -0.091501 -0.008378  0.070137 -0.081332 -0.127002      1  \n",
      "2    -0.185343  0.093481  0.000915 -0.023879 -0.075732 -0.001776     -1  \n",
      "3    -0.113714  0.060630 -0.003223  0.148970 -0.073068 -0.115072      1  \n",
      "4    -0.110030  0.007436 -0.005504 -0.002162 -0.018284 -0.175060     -1  \n",
      "...        ...       ...       ...       ...       ...       ...    ...  \n",
      "1205 -0.110935 -0.009090 -0.006799 -0.015400 -0.005489 -0.150056      1  \n",
      "1206 -0.029822 -0.022555 -0.001477 -0.026160  0.029312  0.023560      1  \n",
      "1207  0.105775 -0.072575 -0.005269  0.054893  0.042350 -0.007926     -1  \n",
      "1208 -0.025273  0.063907 -0.002002 -0.246039 -0.116362  0.102911      1  \n",
      "1209 -0.163917  0.073663 -0.004696  0.094360  0.000111 -0.023285     -1  \n",
      "\n",
      "[1210 rows x 301 columns]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============\n",
      "logme score (GIN supervised_edgepred.pth): 3.800035703818971\n",
      "emb_df: \n",
      "          emb1      emb2      emb3      emb4      emb5      emb6      emb7  \\\n",
      "0     0.018853 -0.006671  0.062913 -0.080721 -0.027570 -0.046811  0.076543   \n",
      "1    -0.097652 -0.083461  0.022607 -0.149393 -0.027918  0.059638  0.079482   \n",
      "2     0.051145 -0.105421  0.080007 -0.029230 -0.063557 -0.022076 -0.047856   \n",
      "3    -0.066924  0.003461  0.025916 -0.122576 -0.028106 -0.090147 -0.041293   \n",
      "4    -0.037802  0.054739  0.081782  0.034592  0.006986  0.079678  0.010378   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "1205 -0.166366  0.035835  0.119096  0.055848 -0.027947  0.054147 -0.007061   \n",
      "1206  0.214898 -0.002941  0.000593 -0.020288 -0.014719  0.081850 -0.043649   \n",
      "1207 -0.213246  0.105717  0.117629 -0.131601 -0.106433 -0.179815  0.111670   \n",
      "1208  0.018054  0.089764  0.038966 -0.121769  0.055908  0.026721 -0.182887   \n",
      "1209  0.078399  0.064981 -0.022349 -0.001778  0.074161 -0.222432 -0.049890   \n",
      "\n",
      "          emb8      emb9     emb10  ...    emb292    emb293    emb294  \\\n",
      "0     0.029893 -0.084239  0.065375  ... -0.012037  0.021982  0.010764   \n",
      "1     0.080519 -0.041539  0.133977  ... -0.014399 -0.014813  0.003126   \n",
      "2     0.082792  0.072304  0.048695  ...  0.012021  0.189639 -0.098478   \n",
      "3     0.076390  0.032185 -0.154962  ... -0.024277  0.055065  0.111219   \n",
      "4    -0.107516  0.013963 -0.047170  ... -0.005136  0.044044 -0.021704   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "1205 -0.114720 -0.012410 -0.008669  ... -0.004823  0.047637  0.023472   \n",
      "1206 -0.136002 -0.034286  0.117832  ... -0.002398 -0.035830 -0.212158   \n",
      "1207 -0.040016 -0.135400  0.041914  ... -0.003190 -0.150062  0.040666   \n",
      "1208 -0.097127 -0.147102  0.073827  ... -0.021424 -0.106852 -0.100466   \n",
      "1209 -0.053601 -0.029135 -0.186097  ...  0.007517 -0.037565  0.010152   \n",
      "\n",
      "        emb295    emb296    emb297    emb298    emb299    emb300  label  \n",
      "0    -0.154799 -0.019885  0.165735 -0.098105  0.003235 -0.028255     -1  \n",
      "1     0.000667 -0.012098  0.171816  0.036384  0.005290  0.192902      1  \n",
      "2     0.076200 -0.021409 -0.012394  0.062237  0.042306  0.127810     -1  \n",
      "3     0.016739 -0.018839  0.088231 -0.029259 -0.052491 -0.061012      1  \n",
      "4     0.014272 -0.013003 -0.011155  0.079381  0.135030  0.044245     -1  \n",
      "...        ...       ...       ...       ...       ...       ...    ...  \n",
      "1205 -0.025278 -0.011443 -0.013469  0.056687  0.137165 -0.033255      1  \n",
      "1206  0.088653 -0.024871  0.062601  0.054277 -0.039222  0.132565      1  \n",
      "1207 -0.022991 -0.018259  0.184469 -0.058897 -0.206488  0.052712     -1  \n",
      "1208 -0.090376 -0.019170  0.038563 -0.093264 -0.104127 -0.108373      1  \n",
      "1209 -0.032263 -0.026619  0.066017 -0.005374 -0.073907  0.021246     -1  \n",
      "\n",
      "[1210 rows x 301 columns]\n",
      "\n",
      "\n",
      "==============\n",
      "logme score (GIN supervised_masking.pth): 3.7818508846255057\n",
      "emb_df: \n",
      "          emb1      emb2      emb3      emb4      emb5      emb6      emb7  \\\n",
      "0    -0.022080 -0.063760  0.120975  0.178429 -0.040215  0.165414  0.000105   \n",
      "1    -0.025957  0.052551  0.045222 -0.029028 -0.042737  0.044182 -0.013683   \n",
      "2    -0.004951 -0.042340  0.110891  0.042711  0.009975 -0.019051  0.014486   \n",
      "3    -0.031390  0.035149 -0.199212  0.231737 -0.066816 -0.075607 -0.008347   \n",
      "4    -0.039251 -0.017963  0.070143 -0.099163 -0.190282  0.017235 -0.022713   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "1205 -0.032213 -0.066142  0.070728 -0.074349 -0.226650  0.014507 -0.018342   \n",
      "1206 -0.008744  0.138530  0.112921  0.044726 -0.040768 -0.045237 -0.003541   \n",
      "1207  0.014146 -0.035452 -0.031068 -0.130117  0.082966  0.075801  0.024097   \n",
      "1208 -0.039803 -0.027897  0.111347  0.098298  0.013565  0.083173 -0.021857   \n",
      "1209 -0.015406  0.169701 -0.077605  0.076964  0.042337  0.005378  0.013477   \n",
      "\n",
      "          emb8      emb9     emb10  ...    emb292    emb293    emb294  \\\n",
      "0     0.047732  0.026219 -0.007856  ... -0.004542  0.037098  0.073299   \n",
      "1     0.048182 -0.147907 -0.017326  ... -0.002123  0.055913  0.030079   \n",
      "2    -0.081182  0.035561  0.001305  ... -0.004142  0.025526  0.043131   \n",
      "3     0.261129  0.042034 -0.009183  ... -0.003261  0.032825 -0.005206   \n",
      "4    -0.010521 -0.056373 -0.024895  ... -0.002652  0.038136 -0.044039   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "1205  0.042133 -0.094672 -0.024153  ... -0.003301  0.032239 -0.039232   \n",
      "1206  0.149110 -0.026034 -0.013779  ... -0.002868  0.040919 -0.001999   \n",
      "1207  0.001420 -0.091648  0.014381  ... -0.006137  0.024916  0.070291   \n",
      "1208 -0.041040 -0.087417 -0.021841  ... -0.002804  0.043125 -0.022974   \n",
      "1209  0.116647 -0.003584 -0.008570  ... -0.004687  0.043441  0.072502   \n",
      "\n",
      "        emb295    emb296    emb297    emb298    emb299    emb300  label  \n",
      "0    -0.009677  0.011228 -0.008657 -0.005822  0.057830  0.179315     -1  \n",
      "1    -0.010989 -0.078054  0.116186 -0.059273 -0.039419 -0.011967      1  \n",
      "2    -0.010052 -0.042777  0.055877  0.031404 -0.044460 -0.183851     -1  \n",
      "3    -0.010520 -0.046050 -0.091172 -0.065239  0.043832  0.169328      1  \n",
      "4    -0.011711 -0.114028 -0.093020 -0.248449  0.034762  0.042605     -1  \n",
      "...        ...       ...       ...       ...       ...       ...    ...  \n",
      "1205 -0.011719 -0.083680 -0.081800 -0.260493  0.072267  0.103245      1  \n",
      "1206 -0.010454 -0.056285 -0.126162  0.079604 -0.003862  0.041389      1  \n",
      "1207 -0.008499  0.141147  0.180943  0.140299 -0.113387  0.017344     -1  \n",
      "1208 -0.010717  0.043423 -0.185974 -0.023969 -0.247615 -0.181718      1  \n",
      "1209 -0.009644 -0.008764 -0.014262  0.058032 -0.011392  0.277675     -1  \n",
      "\n",
      "[1210 rows x 301 columns]\n",
      "\n",
      "\n",
      "==============\n",
      "logme score (GIN supervised_contextpred.pth): 3.794460844019426\n",
      "emb_df: \n",
      "          emb1      emb2      emb3      emb4      emb5      emb6      emb7  \\\n",
      "0    -0.001640 -0.002399  0.000279 -0.315870 -0.001370 -0.000504 -0.075855   \n",
      "1    -0.000430 -0.002133  0.000985 -1.111214 -0.002155 -0.000804 -0.587857   \n",
      "2    -0.000256 -0.002496 -0.000313 -0.354987 -0.000671  0.001824 -0.084763   \n",
      "3    -0.000673 -0.001733  0.000169 -0.439829 -0.001511 -0.000985 -0.506695   \n",
      "4    -0.000052 -0.001988  0.000261 -0.219834 -0.001101 -0.000081 -0.339161   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "1205 -0.000201 -0.001780 -0.000002  0.065339 -0.001348  0.000281 -0.306398   \n",
      "1206  0.000683  0.000181  0.000488 -0.473223 -0.001114 -0.001088 -0.211781   \n",
      "1207 -0.001206 -0.001521 -0.000608 -0.579014 -0.002171 -0.000439 -0.044006   \n",
      "1208  0.000413 -0.001600  0.000166 -0.445116 -0.001425 -0.000196 -0.411879   \n",
      "1209 -0.000791 -0.002121 -0.000014 -0.132536 -0.000996 -0.000088 -0.362479   \n",
      "\n",
      "          emb8      emb9     emb10  ...    emb292    emb293    emb294  \\\n",
      "0    -2.896695  0.001590 -0.001454  ... -0.003193 -2.026291  0.002409   \n",
      "1    -0.776046  0.001580 -0.001361  ... -0.003954 -2.404996  0.001368   \n",
      "2    -0.982630  0.001713 -0.001756  ... -0.001709 -1.751314  0.003533   \n",
      "3    -0.716104  0.002039 -0.000985  ... -0.005036 -0.480977  0.002744   \n",
      "4    -0.666811  0.002471 -0.001322  ... -0.010230 -0.817286  0.001617   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "1205 -0.663619  0.002486 -0.001378  ... -0.011942 -0.879887  0.001706   \n",
      "1206 -1.006782  0.002357 -0.001414  ... -0.004198 -1.884205  0.002271   \n",
      "1207 -3.239683  0.002235 -0.000758  ... -0.003163 -2.530471  0.001875   \n",
      "1208 -0.637260  0.002255 -0.001762  ... -0.011936 -0.615850  0.001447   \n",
      "1209 -0.586027  0.001706 -0.000853  ... -0.003687 -0.521264  0.002038   \n",
      "\n",
      "        emb295    emb296    emb297    emb298    emb299    emb300  label  \n",
      "0     0.010059 -0.003703 -0.001148 -0.997831 -0.001315 -1.676055     -1  \n",
      "1     0.083320 -0.003651 -0.000847 -1.975249 -0.002578 -1.669231      1  \n",
      "2     0.030125 -0.003597 -0.000170 -0.826570  0.000292 -2.872787     -1  \n",
      "3     0.298474 -0.003579 -0.000200 -0.777617 -0.000223 -0.804377      1  \n",
      "4     3.000458 -0.003516 -0.000330 -3.176282 -0.001453 -0.518609     -1  \n",
      "...        ...       ...       ...       ...       ...       ...    ...  \n",
      "1205 -0.069693 -0.003501 -0.000038 -3.074973 -0.001010 -0.516946      1  \n",
      "1206 -0.226131 -0.003480 -0.001659 -0.359993 -0.001285 -1.453040      1  \n",
      "1207 -0.128426 -0.003575  0.000076 -3.698064 -0.000543 -0.797637     -1  \n",
      "1208  0.244305 -0.003493 -0.001082 -1.151466 -0.001240 -0.482500      1  \n",
      "1209  3.525237 -0.003571 -0.000635 -1.266232 -0.001275 -0.985774     -1  \n",
      "\n",
      "[1210 rows x 301 columns]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============\n",
      "logme score (GIN infomax.pth): 3.776506281987087\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"bace\"\n",
    "num_tasks = 1\n",
    "batch_size = 32 # default\n",
    "num_workers = 4 # default\n",
    "save_results_to = '/mnt/sdc/course-projects/GRL-course-project/results'\n",
    "\n",
    "dataset = MoleculeDataset(\"dataset/\" + dataset_name, dataset=dataset_name)\n",
    "print(dataset)\n",
    "\n",
    "# scaffold split:\n",
    "smiles_list = pd.read_csv('dataset/' + dataset_name + '/processed/smiles.csv', header=None)[0].tolist()\n",
    "train_dataset, valid_dataset, test_dataset = scaffold_split(dataset, smiles_list, null_value=0, frac_train=0.8,frac_valid=0.1, frac_test=0.1)\n",
    "print(train_dataset[0])\n",
    "\n",
    "bace_train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers = num_workers)\n",
    "# bace_val_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers = num_workers)\n",
    "# bace_test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers = num_workers)\n",
    "\n",
    "###########################\n",
    "\n",
    "## using GIN supervised model ('./model_gin/supervised.pth') ## \n",
    "\n",
    "bace_graph_features, bace_graph_labels = get_graph_features_labels(bace_train_loader, gin_supervised_model, seed)\n",
    "\n",
    "create_dataframe_save_to_csv(bace_graph_features, bace_graph_labels, dataset_name, \n",
    "                             model_name='gin_supervised', \n",
    "                             save_path=save_results_to)\n",
    "\n",
    "logme = LogME(regression=False)\n",
    "# f has shape of [N, D], y has shape [N]\n",
    "score = logme.fit(np.array(bace_graph_features), np.array(bace_graph_labels))\n",
    "print(\"\\n==============\")\n",
    "print(\"logme score (GIN supervised.pth): {}\".format(score))\n",
    "\n",
    "#############################\n",
    "\n",
    "## using GIN supervised infomax model (supervised_infomax.pth) ## \n",
    "\n",
    "bace_graph_features, bace_graph_labels = get_graph_features_labels(bace_train_loader, gin_supervised_infomax_model, seed)\n",
    "\n",
    "create_dataframe_save_to_csv(bace_graph_features, bace_graph_labels, dataset_name, \n",
    "                             model_name='gin_supervised_infomax', \n",
    "                             save_path=save_results_to)\n",
    "\n",
    "logme = LogME(regression=False)\n",
    "# f has shape of [N, D], y has shape [N]\n",
    "score = logme.fit(np.array(bace_graph_features), np.array(bace_graph_labels))\n",
    "print(\"\\n==============\")\n",
    "print(\"logme score (GIN supervised_infomax.pth): {}\".format(score))\n",
    "\n",
    "#############################\n",
    "\n",
    "## using GIN supervised edgepred model (supervised_edgepred.pth) ## \n",
    "\n",
    "bace_graph_features, bace_graph_labels = get_graph_features_labels(bace_train_loader, gin_supervised_edgepred_model, seed)\n",
    "\n",
    "create_dataframe_save_to_csv(bace_graph_features, bace_graph_labels, dataset_name, \n",
    "                             model_name='gin_supervised_edgepred', \n",
    "                             save_path=save_results_to)\n",
    "\n",
    "logme = LogME(regression=False)\n",
    "# f has shape of [N, D], y has shape [N]\n",
    "score = logme.fit(np.array(bace_graph_features), np.array(bace_graph_labels))\n",
    "print(\"\\n==============\")\n",
    "print(\"logme score (GIN supervised_edgepred.pth): {}\".format(score))\n",
    "\n",
    "#############################\n",
    "\n",
    "## using GIN supervised masking model (supervised_masking.pth) ## \n",
    "\n",
    "bace_graph_features, bace_graph_labels = get_graph_features_labels(bace_train_loader, gin_supervised_masking_model, seed)\n",
    "\n",
    "create_dataframe_save_to_csv(bace_graph_features, bace_graph_labels, dataset_name, \n",
    "                             model_name='gin_supervised_masking', \n",
    "                             save_path=save_results_to)\n",
    "\n",
    "logme = LogME(regression=False)\n",
    "# f has shape of [N, D], y has shape [N]\n",
    "score = logme.fit(np.array(bace_graph_features), np.array(bace_graph_labels))\n",
    "print(\"\\n==============\")\n",
    "print(\"logme score (GIN supervised_masking.pth): {}\".format(score))\n",
    "\n",
    "#############################\n",
    "\n",
    "## using GIN supervised contextpred model (supervised_contextpred.pth) ## \n",
    "\n",
    "bace_graph_features, bace_graph_labels = get_graph_features_labels(bace_train_loader, gin_supervised_contextpred_model, seed)\n",
    "\n",
    "create_dataframe_save_to_csv(bace_graph_features, bace_graph_labels, dataset_name, \n",
    "                             model_name='gin_supervised_contextpred', \n",
    "                             save_path=save_results_to)\n",
    "\n",
    "logme = LogME(regression=False)\n",
    "# f has shape of [N, D], y has shape [N]\n",
    "score = logme.fit(np.array(bace_graph_features), np.array(bace_graph_labels))\n",
    "print(\"\\n==============\")\n",
    "print(\"logme score (GIN supervised_contextpred.pth): {}\".format(score))\n",
    "\n",
    "#############################\n",
    "\n",
    "## using GIN infomax model (infomax.pth) ## \n",
    "\n",
    "bace_graph_features, bace_graph_labels = get_graph_features_labels(bace_train_loader, gin_infomax_model, seed)\n",
    "\n",
    "create_dataframe_save_to_csv(bace_graph_features, bace_graph_labels, dataset_name, \n",
    "                             model_name='gin_infomax', \n",
    "                             save_path=save_results_to)\n",
    "\n",
    "logme = LogME(regression=False)\n",
    "# f has shape of [N, D], y has shape [N]\n",
    "score = logme.fit(np.array(bace_graph_features), np.array(bace_graph_labels))\n",
    "print(\"\\n==============\")\n",
    "print(\"logme score (GIN infomax.pth): {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beed3de1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
