{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93100a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric\n",
    "\n",
    "from torch_geometric.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from model import GNN, GNN_graphpred\n",
    "from splitters import random_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b0e376f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_geometric.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3208bf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code copied from: https://github.com/pyg-team/pytorch_geometric/blob/90a06d02b79414fc48a3367c79cfb28f919d5769/torch_geometric/datasets/tu_dataset.py ## \n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset, download_url, extract_zip\n",
    "from torch_geometric.read import read_tu_data\n",
    "\n",
    "\n",
    "class TUDataset(InMemoryDataset):\n",
    "    r\"\"\"A variety of graph kernel benchmark datasets, *.e.g.* \"IMDB-BINARY\",\n",
    "    \"REDDIT-BINARY\" or \"PROTEINS\", collected from the `TU Dortmund University\n",
    "    <http://graphkernels.cs.tu-dortmund.de>`_.\n",
    "    Args:\n",
    "        root (string): Root directory where the dataset should be saved.\n",
    "        name (string): The `name <http://graphkernels.cs.tu-dortmund.de>`_ of\n",
    "            the dataset.\n",
    "        transform (callable, optional): A function/transform that takes in an\n",
    "            :obj:`torch_geometric.data.Data` object and returns a transformed\n",
    "            version. The data object will be transformed before every access.\n",
    "            (default: :obj:`None`)\n",
    "        pre_transform (callable, optional): A function/transform that takes in\n",
    "            an :obj:`torch_geometric.data.Data` object and returns a\n",
    "            transformed version. The data object will be transformed before\n",
    "            being saved to disk. (default: :obj:`None`)\n",
    "        pre_filter (callable, optional): A function that takes in an\n",
    "            :obj:`torch_geometric.data.Data` object and returns a boolean\n",
    "            value, indicating whether the data object should be included in the\n",
    "            final dataset. (default: :obj:`None`)\n",
    "        use_node_attr (bool, optional): If :obj:`True`, the dataset will\n",
    "            contain additional continuous node features (if present).\n",
    "            (default: :obj:`False`)\n",
    "    \"\"\"\n",
    "\n",
    "    url = 'https://ls11-www.cs.tu-dortmund.de/people/morris/' \\\n",
    "          'graphkerneldatasets'\n",
    "\n",
    "    def __init__(self,\n",
    "                 root,\n",
    "                 name,\n",
    "                 transform=None,\n",
    "                 pre_transform=None,\n",
    "                 pre_filter=None,\n",
    "                 use_node_attr=False):\n",
    "        self.name = name\n",
    "        super(TUDataset, self).__init__(root, transform, pre_transform,\n",
    "                                        pre_filter)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        if self.data.x is not None and not use_node_attr:\n",
    "            self.data.x = self.data.x[:, self.num_node_attributes:]\n",
    "\n",
    "    @property\n",
    "    def num_node_labels(self):\n",
    "        if self.data.x is None:\n",
    "            return 0\n",
    "\n",
    "        for i in range(self.data.x.size(1)):\n",
    "            if self.data.x[:, i:].sum().item() == self.data.x.size(0):\n",
    "                return self.data.x.size(1) - i\n",
    "\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def num_node_attributes(self):\n",
    "        if self.data.x is None:\n",
    "            return 0\n",
    "\n",
    "        return self.data.x.size(1) - self.num_node_labels\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        names = ['A', 'graph_indicator']\n",
    "        return ['{}_{}.txt'.format(self.name, name) for name in names]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return 'data.pt'\n",
    "\n",
    "    def download(self):\n",
    "        path = download_url('{}/{}.zip'.format(self.url, self.name), self.root)\n",
    "        extract_zip(path, self.root)\n",
    "        os.unlink(path)\n",
    "        shutil.rmtree(self.raw_dir)\n",
    "        os.rename(osp.join(self.root, self.name), self.raw_dir)\n",
    "\n",
    "    def process(self):\n",
    "        self.data, self.slices = read_tu_data(self.raw_dir, self.name)\n",
    "\n",
    "        if self.pre_filter is not None:\n",
    "            data_list = [self.get(idx) for idx in range(len(self))]\n",
    "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
    "            self.data, self.slices = self.collate(data_list)\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.get(idx) for idx in range(len(self))]\n",
    "            data_list = [self.pre_transform(data) for data in data_list]\n",
    "            self.data, self.slices = self.collate(data_list)\n",
    "\n",
    "        torch.save((self.data, self.slices), self.processed_paths[0])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({})'.format(self.name, len(self))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9afd8db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from https://github.com/thuml/LogME/blob/main/LogME.py\n",
    "## NOTE: commented out njit because that package takes forever to load in jupyter notebook\n",
    "\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "# from numba import njit\n",
    "\n",
    "# @njit\n",
    "def each_evidence(y_, f, fh, v, s, vh, N, D):\n",
    "    \"\"\"\n",
    "    compute the maximum evidence for each class\n",
    "    \"\"\"\n",
    "    epsilon = 1e-5\n",
    "    alpha = 1.0\n",
    "    beta = 1.0\n",
    "    lam = alpha / beta\n",
    "    tmp = (vh @ (f @ np.ascontiguousarray(y_)))\n",
    "    for _ in range(11):\n",
    "        # should converge after at most 10 steps\n",
    "        # typically converge after two or three steps\n",
    "        gamma = (s / (s + lam)).sum()\n",
    "        # A = v @ np.diag(alpha + beta * s) @ v.transpose() # no need to compute A\n",
    "        # A_inv = v @ np.diag(1.0 / (alpha + beta * s)) @ v.transpose() # no need to compute A_inv\n",
    "        m = v @ (tmp * beta / (alpha + beta * s))\n",
    "        alpha_de = (m * m).sum()\n",
    "        alpha = gamma / (alpha_de + epsilon)\n",
    "        beta_de = ((y_ - fh @ m) ** 2).sum()\n",
    "        beta = (N - gamma) / (beta_de + epsilon)\n",
    "        new_lam = alpha / beta\n",
    "        if np.abs(new_lam - lam) / lam < 0.01:\n",
    "            break\n",
    "        lam = new_lam\n",
    "    evidence = D / 2.0 * np.log(alpha) \\\n",
    "               + N / 2.0 * np.log(beta) \\\n",
    "               - 0.5 * np.sum(np.log(alpha + beta * s)) \\\n",
    "               - beta / 2.0 * (beta_de + epsilon) \\\n",
    "               - alpha / 2.0 * (alpha_de + epsilon) \\\n",
    "               - N / 2.0 * np.log(2 * np.pi)\n",
    "    return evidence / N, alpha, beta, m\n",
    "\n",
    "\n",
    "# use pseudo data to compile the function\n",
    "# D = 20, N = 50\n",
    "f_tmp = np.random.randn(20, 50).astype(np.float64)\n",
    "each_evidence(np.random.randint(0, 2, 50).astype(np.float64), f_tmp, f_tmp.transpose(), np.eye(20, dtype=np.float64), np.ones(20, dtype=np.float64), np.eye(20, dtype=np.float64), 50, 20)\n",
    "\n",
    "\n",
    "# @njit\n",
    "def truncated_svd(x):\n",
    "    u, s, vh = np.linalg.svd(x.transpose() @ x)\n",
    "    s = np.sqrt(s)\n",
    "    u_times_sigma = x @ vh.transpose()\n",
    "    k = np.sum((s > 1e-10) * 1)  # rank of f\n",
    "    s = s.reshape(-1, 1)\n",
    "    s = s[:k]\n",
    "    vh = vh[:k]\n",
    "    u = u_times_sigma[:, :k] / s.reshape(1, -1)\n",
    "    return u, s, vh\n",
    "truncated_svd(np.random.randn(20, 10).astype(np.float64))\n",
    "\n",
    "\n",
    "class LogME(object):\n",
    "    def __init__(self, regression=False):\n",
    "        \"\"\"\n",
    "            :param regression: whether regression\n",
    "        \"\"\"\n",
    "        self.regression = regression\n",
    "        self.fitted = False\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.num_dim = 0\n",
    "        self.alphas = []  # alpha for each class / dimension\n",
    "        self.betas = []  # beta for each class / dimension\n",
    "        # self.ms.shape --> [C, D]\n",
    "        self.ms = []  # m for each class / dimension\n",
    "\n",
    "    def _fit_icml(self, f: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        LogME calculation proposed in the ICML 2021 paper\n",
    "        \"LogME: Practical Assessment of Pre-trained Models for Transfer Learning\"\n",
    "        at http://proceedings.mlr.press/v139/you21b.html\n",
    "        \"\"\"\n",
    "        fh = f\n",
    "        f = f.transpose()\n",
    "        D, N = f.shape\n",
    "        v, s, vh = np.linalg.svd(f @ fh, full_matrices=True)\n",
    "\n",
    "        evidences = []\n",
    "        self.num_dim = y.shape[1] if self.regression else int(y.max() + 1)\n",
    "        for i in range(self.num_dim):\n",
    "            y_ = y[:, i] if self.regression else (y == i).astype(np.float64)\n",
    "            evidence, alpha, beta, m = each_evidence(y_, f, fh, v, s, vh, N, D)\n",
    "            evidences.append(evidence)\n",
    "            self.alphas.append(alpha)\n",
    "            self.betas.append(beta)\n",
    "            self.ms.append(m)\n",
    "        self.ms = np.stack(self.ms)\n",
    "        return np.mean(evidences)\n",
    "\n",
    "    def _fit_fixed_point(self, f: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        LogME calculation proposed in the arxiv 2021 paper\n",
    "        \"Ranking and Tuning Pre-trained Models: A New Paradigm of Exploiting Model Hubs\"\n",
    "        at https://arxiv.org/abs/2110.10545\n",
    "        \"\"\"\n",
    "        N, D = f.shape  # k = min(N, D)\n",
    "        if N > D: # direct SVD may be expensive\n",
    "            u, s, vh = truncated_svd(f)\n",
    "        else:\n",
    "            u, s, vh = np.linalg.svd(f, full_matrices=False)\n",
    "        # u.shape = N x k\n",
    "        # s.shape = k\n",
    "        # vh.shape = k x D\n",
    "        s = s.reshape(-1, 1)\n",
    "        sigma = (s ** 2)\n",
    "\n",
    "        evidences = []\n",
    "        self.num_dim = y.shape[1] if self.regression else int(y.max() + 1)\n",
    "        for i in range(self.num_dim):\n",
    "            y_ = y[:, i] if self.regression else (y == i).astype(np.float64)\n",
    "            y_ = y_.reshape(-1, 1)\n",
    "            x = u.T @ y_  # x has shape [k, 1], but actually x should have shape [N, 1]\n",
    "            x2 = x ** 2\n",
    "            res_x2 = (y_ ** 2).sum() - x2.sum()  # if k < N, we compute sum of xi for 0 singular values directly\n",
    "\n",
    "            alpha, beta = 1.0, 1.0\n",
    "            for _ in range(11):\n",
    "                t = alpha / beta\n",
    "                gamma = (sigma / (sigma + t)).sum()\n",
    "                m2 = (sigma * x2 / ((t + sigma) ** 2)).sum()\n",
    "                res2 = (x2 / ((1 + sigma / t) ** 2)).sum() + res_x2\n",
    "                alpha = gamma / (m2 + 1e-5)\n",
    "                beta = (N - gamma) / (res2 + 1e-5)\n",
    "                t_ = alpha / beta\n",
    "                evidence = D / 2.0 * np.log(alpha) \\\n",
    "                           + N / 2.0 * np.log(beta) \\\n",
    "                           - 0.5 * np.sum(np.log(alpha + beta * sigma)) \\\n",
    "                           - beta / 2.0 * res2 \\\n",
    "                           - alpha / 2.0 * m2 \\\n",
    "                           - N / 2.0 * np.log(2 * np.pi)\n",
    "                evidence /= N\n",
    "                if abs(t_ - t) / t <= 1e-3:  # abs(t_ - t) <= 1e-5 or abs(1 / t_ - 1 / t) <= 1e-5:\n",
    "                    break\n",
    "            evidence = D / 2.0 * np.log(alpha) \\\n",
    "                       + N / 2.0 * np.log(beta) \\\n",
    "                       - 0.5 * np.sum(np.log(alpha + beta * sigma)) \\\n",
    "                       - beta / 2.0 * res2 \\\n",
    "                       - alpha / 2.0 * m2 \\\n",
    "                       - N / 2.0 * np.log(2 * np.pi)\n",
    "            evidence /= N\n",
    "            m = 1.0 / (t + sigma) * s * x\n",
    "            m = (vh.T @ m).reshape(-1)\n",
    "            evidences.append(evidence)\n",
    "            self.alphas.append(alpha)\n",
    "            self.betas.append(beta)\n",
    "            self.ms.append(m)\n",
    "        self.ms = np.stack(self.ms)\n",
    "        return np.mean(evidences)\n",
    "\n",
    "    _fit = _fit_fixed_point\n",
    "\n",
    "    def fit(self, f: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        :param f: [N, F], feature matrix from pre-trained model\n",
    "        :param y: target labels.\n",
    "            For classification, y has shape [N] with element in [0, C_t).\n",
    "            For regression, y has shape [N, C] with C regression-labels\n",
    "        :return: LogME score (how well f can fit y directly)\n",
    "        \"\"\"\n",
    "        if self.fitted:\n",
    "            warnings.warn('re-fitting for new data. old parameters cleared.')\n",
    "            self.reset()\n",
    "        else:\n",
    "            self.fitted = True\n",
    "        f = f.astype(np.float64)\n",
    "        if self.regression:\n",
    "            y = y.astype(np.float64)\n",
    "            if len(y.shape) == 1:\n",
    "                y = y.reshape(-1, 1)\n",
    "        return self._fit(f, y)\n",
    "\n",
    "    def predict(self, f: np.ndarray):\n",
    "        \"\"\"\n",
    "        :param f: [N, F], feature matrix\n",
    "        :return: prediction, return shape [N, X]\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"not fitted, please call fit first\")\n",
    "        f = f.astype(np.float64)\n",
    "        logits = f @ self.ms.T\n",
    "        if self.regression:\n",
    "            return logits\n",
    "        return np.argmax(logits, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ebe2fb",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5958e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_features_labels_imdb(loader, model, seed):\n",
    "    \"\"\"Extract graph features for IMDB graph data.\n",
    "    Note: \n",
    "        Fake node features ([0, 0]) and fake edge attributes ([0, 0])\n",
    "        are created for each node and edge so we can use the pre-trained \n",
    "        Strategies (Hu et al., ICLR 2020) GNNs.\n",
    "        \n",
    "    Args:\n",
    "        loader : IMDB dataloader\n",
    "        model : pre-trained GNN model\n",
    "        seed : integer value random seed value\n",
    "    \n",
    "    Returns:\n",
    "        all_graph_features : list of all graph features from the dataloader\n",
    "        all_graph_labels : list of all graph labels from the dataloader \n",
    "    \"\"\"\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    all_graph_features = []\n",
    "    all_graph_labels = []\n",
    "    \n",
    "    for step, batch in enumerate(loader):\n",
    "#         print(\"step: {}\".format(step))\n",
    "#         print(\"batch: {}\".format(batch))\n",
    "#         print(\"batch.batch.shape: {}\".format(batch.batch.shape))\n",
    "        num_nodes = batch.batch.shape[0]\n",
    "        num_edges = batch.edge_index.shape[1]\n",
    "#         print(\"num_nodes: {}\".format(num_nodes))\n",
    "#         print(\"num_edges: {}\".format(num_edges))\n",
    "        batch = batch.to(device)\n",
    "    \n",
    "        # create fake node features [0, 0] for each node of the IMDB dataset\n",
    "        x = torch.zeros(size=[num_nodes, 2], dtype=torch.long).to(device)\n",
    "\n",
    "        # create fake edge attribute [0, 0] for each edge of the IMDB dataset\n",
    "        edge_attr = torch.zeros(size=[num_edges, 2], dtype=torch.long).to(device)\n",
    "\n",
    "        y = batch.y\n",
    "        edge_index = batch.edge_index\n",
    "        batch = batch.batch\n",
    "\n",
    "        node_representation = model.gnn(x, edge_index, edge_attr)\n",
    "\n",
    "        graph_features = model.pool(node_representation, batch)\n",
    "#         print(\"graph_features: {}\".format(graph_features))\n",
    "#         print(\"graph_features.shape: {}\".format(graph_features.shape)) # batch_size x outdim (300)\n",
    "        all_graph_features.extend(graph_features.cpu().detach().numpy())\n",
    "        all_graph_labels.extend(y.cpu().detach().numpy())\n",
    "        \n",
    "    return all_graph_features, all_graph_labels\n",
    "\n",
    "\n",
    "def create_dataframe_save_to_csv(embeddings, labels, dataset_name, model_name, save_path):\n",
    "#     ## create pandas dataframe to store: example id, embeddings, labels ## \n",
    "#     d = {'example_id': [i for i in range(len(embeddings))],\n",
    "#             'embeddings': embeddings,\n",
    "#             'labels': labels\n",
    "#            }\n",
    "#     df = pd.DataFrame(data=d)\n",
    "#     # df.head(15)\n",
    "\n",
    "#     if not os.path.exists(save_path):\n",
    "#         os.makedirs(save_path)\n",
    "\n",
    "    filename = '{}_{}.csv'.format(dataset_name, model_name)\n",
    "#     print(\"dataset_name: {}\".format(dataset_name))\n",
    "#     df.to_csv(os.path.join(save_path, filename), index=False)\n",
    "    emb_df = pd.DataFrame(np.array(embeddings))\n",
    "    emb_df.columns = ['emb' + str(e+1) for e in range(emb_df.shape[1])]\n",
    "    emb_df['label'] = labels\n",
    "#     print(\"emb_df: \\n{}\\n\".format(emb_df))\n",
    "    emb_df.to_csv(os.path.join(save_path, filename), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7b0513",
   "metadata": {},
   "source": [
    "# Default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54342370",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GNN default values ## \n",
    "num_layer = 5 # default\n",
    "emb_dim = 300 # default\n",
    "JK = 'last' # default (how the node features across laysers are combined)\n",
    "dropout_ratio = 0.5 # default\n",
    "graph_pooling = 'mean' # default\n",
    "gnn_type = 'gin' # default\n",
    "\n",
    "## DataLoader default values ## \n",
    "batch_size = 32 # strategies default\n",
    "num_workers = 4 # strategies default\n",
    "train_shuffle = False \n",
    "num_tasks = 1\n",
    "\n",
    "\n",
    "## others ## \n",
    "save_results_to = '/mnt/sdc/course-projects/GRL-course-project/results'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0d5d06",
   "metadata": {},
   "source": [
    "# Set up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a47d2d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GNN_graphpred(\n",
       "  (gnn): GNN(\n",
       "    (x_embedding1): Embedding(120, 300)\n",
       "    (x_embedding2): Embedding(3, 300)\n",
       "    (gnns): ModuleList(\n",
       "      (0): GINConv(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=600, out_features=300, bias=True)\n",
       "        )\n",
       "        (edge_embedding1): Embedding(6, 300)\n",
       "        (edge_embedding2): Embedding(3, 300)\n",
       "      )\n",
       "      (1): GINConv(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=600, out_features=300, bias=True)\n",
       "        )\n",
       "        (edge_embedding1): Embedding(6, 300)\n",
       "        (edge_embedding2): Embedding(3, 300)\n",
       "      )\n",
       "      (2): GINConv(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=600, out_features=300, bias=True)\n",
       "        )\n",
       "        (edge_embedding1): Embedding(6, 300)\n",
       "        (edge_embedding2): Embedding(3, 300)\n",
       "      )\n",
       "      (3): GINConv(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=600, out_features=300, bias=True)\n",
       "        )\n",
       "        (edge_embedding1): Embedding(6, 300)\n",
       "        (edge_embedding2): Embedding(3, 300)\n",
       "      )\n",
       "      (4): GINConv(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=600, out_features=300, bias=True)\n",
       "        )\n",
       "        (edge_embedding1): Embedding(6, 300)\n",
       "        (edge_embedding2): Embedding(3, 300)\n",
       "      )\n",
       "    )\n",
       "    (batch_norms): ModuleList(\n",
       "      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Set up model ## \n",
    "model = GNN_graphpred(num_layer, emb_dim, num_tasks, JK, dropout_ratio, graph_pooling, gnn_type)\n",
    "\n",
    "##########################\n",
    "input_model_file = './model_gin/supervised.pth'\n",
    "\n",
    "gin_supervised_model = GNN_graphpred(num_layer, emb_dim, num_tasks, JK = JK, drop_ratio = dropout_ratio, graph_pooling = graph_pooling, gnn_type = gnn_type)\n",
    "gin_supervised_model.from_pretrained(input_model_file)\n",
    "\n",
    "gin_supervised_model.to(device)\n",
    "gin_supervised_model.eval()\n",
    "\n",
    "###########################\n",
    "\n",
    "input_model_file = './model_gin/supervised_infomax.pth'\n",
    "\n",
    "gin_supervised_infomax_model = GNN_graphpred(num_layer, emb_dim, num_tasks, JK = JK, drop_ratio = dropout_ratio, graph_pooling = graph_pooling, gnn_type = gnn_type)\n",
    "gin_supervised_infomax_model.from_pretrained(input_model_file)\n",
    "\n",
    "gin_supervised_infomax_model.to(device)\n",
    "gin_supervised_infomax_model.eval()\n",
    "\n",
    "###########################\n",
    "\n",
    "input_model_file = './model_gin/supervised_edgepred.pth'\n",
    "\n",
    "gin_supervised_edgepred_model = GNN_graphpred(num_layer, emb_dim, num_tasks, JK = JK, drop_ratio = dropout_ratio, graph_pooling = graph_pooling, gnn_type = gnn_type)\n",
    "gin_supervised_edgepred_model.from_pretrained(input_model_file)\n",
    "\n",
    "gin_supervised_edgepred_model.to(device)\n",
    "gin_supervised_edgepred_model.eval()\n",
    "\n",
    "###########################\n",
    "\n",
    "input_model_file = './model_gin/supervised_masking.pth'\n",
    "\n",
    "gin_supervised_masking_model = GNN_graphpred(num_layer, emb_dim, num_tasks, JK = JK, drop_ratio = dropout_ratio, graph_pooling = graph_pooling, gnn_type = gnn_type)\n",
    "gin_supervised_masking_model.from_pretrained(input_model_file)\n",
    "\n",
    "gin_supervised_masking_model.to(device)\n",
    "gin_supervised_masking_model.eval()\n",
    "\n",
    "###########################\n",
    "\n",
    "input_model_file = './model_gin/supervised_contextpred.pth'\n",
    "\n",
    "gin_supervised_contextpred_model = GNN_graphpred(num_layer, emb_dim, num_tasks, JK = JK, drop_ratio = dropout_ratio, graph_pooling = graph_pooling, gnn_type = gnn_type)\n",
    "gin_supervised_contextpred_model.from_pretrained(input_model_file)\n",
    "\n",
    "gin_supervised_contextpred_model.to(device)\n",
    "gin_supervised_contextpred_model.eval()\n",
    "\n",
    "###########################\n",
    "\n",
    "input_model_file = './model_gin/infomax.pth'\n",
    "\n",
    "gin_infomax_model = GNN_graphpred(num_layer, emb_dim, num_tasks, JK = JK, drop_ratio = dropout_ratio, graph_pooling = graph_pooling, gnn_type = gnn_type)\n",
    "gin_infomax_model.from_pretrained(input_model_file)\n",
    "\n",
    "gin_infomax_model.to(device)\n",
    "gin_infomax_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4395bee",
   "metadata": {},
   "source": [
    "# Load IMDB Dataset + test using Strategies pre-trained GNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863ed911",
   "metadata": {},
   "source": [
    "## IMDB Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96ab19bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dataset downloaded to '/pretrain-gnns-master/chem/data/imdb/binary' ## \n",
    "dataset_name = 'imdbb'\n",
    "imdb_dataset = TUDataset(root='data/imdb/binary', name='IMDB-BINARY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9225276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IMDB-BINARY(1000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d7b865f",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdbb_train_dataset, imdbb_valid_dataset, imdbb_test_dataset = random_split(imdb_dataset, null_value=0, \n",
    "                                                                         frac_train=0.8,\n",
    "                                                                         frac_valid=0.1,\n",
    "                                                                         frac_test=0.1,\n",
    "                                                                         seed=seed)\n",
    "\n",
    "imdbb_train_loader = DataLoader(imdbb_train_dataset, batch_size=batch_size, \n",
    "                               shuffle=train_shuffle, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "704dd64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============\n",
      "logme score (GIN supervised.pth): -0.6136261241956099\n"
     ]
    }
   ],
   "source": [
    "\n",
    "imdbb_graph_features, imdbb_graph_labels = get_graph_features_labels_imdb(imdbb_train_loader, \n",
    "                                                                          gin_supervised_model,\n",
    "                                                                          seed)\n",
    "create_dataframe_save_to_csv(imdbb_graph_features, imdbb_graph_labels, dataset_name,\n",
    "                             model_name='gin_supervised',\n",
    "                             save_path=save_results_to)\n",
    "\n",
    "logme = LogME(regression=False)\n",
    "score = logme.fit(np.array(imdbb_graph_features), np.array(imdbb_graph_labels))\n",
    "print(\"\\n=============\")\n",
    "print(\"logme score (GIN supervised.pth): {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9caefc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============\n",
      "logme score (GIN supervised_infomax.pth): -0.6249411817184796\n"
     ]
    }
   ],
   "source": [
    "imdbb_graph_features, imdbb_graph_labels = get_graph_features_labels_imdb(imdbb_train_loader, \n",
    "                                                                          gin_supervised_infomax_model,\n",
    "                                                                          seed)\n",
    "create_dataframe_save_to_csv(imdbb_graph_features, imdbb_graph_labels, dataset_name,\n",
    "                             model_name='gin_supervised_infomax',\n",
    "                             save_path=save_results_to)\n",
    "\n",
    "logme = LogME(regression=False)\n",
    "score = logme.fit(np.array(imdbb_graph_features), np.array(imdbb_graph_labels))\n",
    "print(\"\\n=============\")\n",
    "print(\"logme score (GIN supervised_infomax.pth): {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "258449f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============\n",
      "logme score (GIN supervised_edgepred.pth): -0.6277998190368825\n"
     ]
    }
   ],
   "source": [
    "imdbb_graph_features, imdbb_graph_labels = get_graph_features_labels_imdb(imdbb_train_loader, \n",
    "                                                                          gin_supervised_edgepred_model,\n",
    "                                                                          seed)\n",
    "create_dataframe_save_to_csv(imdbb_graph_features, imdbb_graph_labels, dataset_name,\n",
    "                             model_name='gin_supervised_edgepred',\n",
    "                             save_path=save_results_to)\n",
    "\n",
    "logme = LogME(regression=False)\n",
    "score = logme.fit(np.array(imdbb_graph_features), np.array(imdbb_graph_labels))\n",
    "print(\"\\n=============\")\n",
    "print(\"logme score (GIN supervised_edgepred.pth): {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "993aea47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============\n",
      "logme score (GIN supervised_masking.pth): -0.6408094598830782\n"
     ]
    }
   ],
   "source": [
    "imdbb_graph_features, imdbb_graph_labels = get_graph_features_labels_imdb(imdbb_train_loader, \n",
    "                                                                          gin_supervised_masking_model,\n",
    "                                                                          seed)\n",
    "create_dataframe_save_to_csv(imdbb_graph_features, imdbb_graph_labels, dataset_name,\n",
    "                             model_name='gin_supervised_masking',\n",
    "                             save_path=save_results_to)\n",
    "\n",
    "logme = LogME(regression=False)\n",
    "score = logme.fit(np.array(imdbb_graph_features), np.array(imdbb_graph_labels))\n",
    "print(\"\\n=============\")\n",
    "print(\"logme score (GIN supervised_masking.pth): {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c64c3f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============\n",
      "logme score (GIN supervised_contextpred.pth): -0.635537474117997\n"
     ]
    }
   ],
   "source": [
    "imdbb_graph_features, imdbb_graph_labels = get_graph_features_labels_imdb(imdbb_train_loader, \n",
    "                                                                          gin_supervised_contextpred_model,\n",
    "                                                                          seed)\n",
    "create_dataframe_save_to_csv(imdbb_graph_features, imdbb_graph_labels, dataset_name,\n",
    "                             model_name='gin_supervised_contextpred',\n",
    "                             save_path=save_results_to)\n",
    "\n",
    "logme = LogME(regression=False)\n",
    "score = logme.fit(np.array(imdbb_graph_features), np.array(imdbb_graph_labels))\n",
    "print(\"\\n=============\")\n",
    "print(\"logme score (GIN supervised_contextpred.pth): {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee025c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============\n",
      "logme score (GIN infomax.pth): -0.6333544027212512\n"
     ]
    }
   ],
   "source": [
    "imdbb_graph_features, imdbb_graph_labels = get_graph_features_labels_imdb(imdbb_train_loader, \n",
    "                                                                          gin_infomax_model,\n",
    "                                                                          seed)\n",
    "create_dataframe_save_to_csv(imdbb_graph_features, imdbb_graph_labels, dataset_name,\n",
    "                             model_name='gin_infomax',\n",
    "                             save_path=save_results_to)\n",
    "\n",
    "logme = LogME(regression=False)\n",
    "score = logme.fit(np.array(imdbb_graph_features), np.array(imdbb_graph_labels))\n",
    "print(\"\\n=============\")\n",
    "print(\"logme score (GIN infomax.pth): {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a00d662",
   "metadata": {},
   "source": [
    "## IMDB Multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5815e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://ls11-www.cs.tu-dortmund.de/people/morris/graphkerneldatasets/IMDB-MULTI.zip\n",
      "Extracting data/imdb/binary/IMDB-MULTI.zip\n"
     ]
    }
   ],
   "source": [
    "## dataset downloaded to '/pretrain-gnns-master/chem/data/imdb/binary' ## \n",
    "dataset_name = 'imdbm'\n",
    "imdb_dataset = TUDataset(root='data/imdb/binary', name='IMDB-MULTI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39e4560c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IMDB-MULTI(1000)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "185524bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdbm_train_dataset, imdbm_valid_dataset, imdbm_test_dataset = random_split(imdb_dataset, null_value=0, \n",
    "                                                                         frac_train=0.8,\n",
    "                                                                         frac_valid=0.1,\n",
    "                                                                         frac_test=0.1,\n",
    "                                                                         seed=seed)\n",
    "\n",
    "imdbm_train_loader = DataLoader(imdbm_train_dataset, batch_size=batch_size, \n",
    "                               shuffle=train_shuffle, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10014c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============\n",
      "logme score (GIN supervised.pth): -0.6136266211400907\n"
     ]
    }
   ],
   "source": [
    "imdbm_graph_features, imdbm_graph_labels = get_graph_features_labels_imdb(imdbm_train_loader, \n",
    "                                                                          gin_supervised_model,\n",
    "                                                                          seed)\n",
    "create_dataframe_save_to_csv(imdbm_graph_features, imdbm_graph_labels, dataset_name,\n",
    "                             model_name='gin_supervised',\n",
    "                             save_path=save_results_to)\n",
    "\n",
    "logme = LogME(regression=False)\n",
    "score = logme.fit(np.array(imdbm_graph_features), np.array(imdbm_graph_labels))\n",
    "print(\"\\n=============\")\n",
    "print(\"logme score (GIN supervised.pth): {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b67d8970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============\n",
      "logme score (GIN supervised_infomax.pth): -0.6249413416397545\n"
     ]
    }
   ],
   "source": [
    "imdbm_graph_features, imdbm_graph_labels = get_graph_features_labels_imdb(imdbm_train_loader, \n",
    "                                                                          gin_supervised_infomax_model,\n",
    "                                                                          seed)\n",
    "create_dataframe_save_to_csv(imdbm_graph_features, imdbm_graph_labels, dataset_name,\n",
    "                             model_name='gin_supervised_infomax',\n",
    "                             save_path=save_results_to)\n",
    "\n",
    "logme = LogME(regression=False)\n",
    "score = logme.fit(np.array(imdbm_graph_features), np.array(imdbm_graph_labels))\n",
    "print(\"\\n=============\")\n",
    "print(\"logme score (GIN supervised_infomax.pth): {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f899ea83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============\n",
      "logme score (GIN supervised_edgepred.pth): -0.6278001157464586\n"
     ]
    }
   ],
   "source": [
    "imdbm_graph_features, imdbm_graph_labels = get_graph_features_labels_imdb(imdbm_train_loader, \n",
    "                                                                          gin_supervised_edgepred_model,\n",
    "                                                                          seed)\n",
    "create_dataframe_save_to_csv(imdbm_graph_features, imdbm_graph_labels, dataset_name,\n",
    "                             model_name='gin_supervised_edgepred',\n",
    "                             save_path=save_results_to)\n",
    "\n",
    "logme = LogME(regression=False)\n",
    "score = logme.fit(np.array(imdbm_graph_features), np.array(imdbm_graph_labels))\n",
    "print(\"\\n=============\")\n",
    "print(\"logme score (GIN supervised_edgepred.pth): {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d3798fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============\n",
      "logme score (GIN supervised_masking.pth): -0.6408094293912805\n"
     ]
    }
   ],
   "source": [
    "imdbm_graph_features, imdbm_graph_labels = get_graph_features_labels_imdb(imdbm_train_loader, \n",
    "                                                                          gin_supervised_masking_model,\n",
    "                                                                          seed)\n",
    "create_dataframe_save_to_csv(imdbm_graph_features, imdbm_graph_labels, dataset_name,\n",
    "                             model_name='gin_supervised_masking',\n",
    "                             save_path=save_results_to)\n",
    "\n",
    "logme = LogME(regression=False)\n",
    "score = logme.fit(np.array(imdbm_graph_features), np.array(imdbm_graph_labels))\n",
    "print(\"\\n=============\")\n",
    "print(\"logme score (GIN supervised_masking.pth): {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "afdfc0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============\n",
      "logme score (GIN supervised_contextpred.pth): -0.6355368522450561\n"
     ]
    }
   ],
   "source": [
    "imdbm_graph_features, imdbm_graph_labels = get_graph_features_labels_imdb(imdbm_train_loader, \n",
    "                                                                          gin_supervised_contextpred_model,\n",
    "                                                                          seed)\n",
    "create_dataframe_save_to_csv(imdbm_graph_features, imdbm_graph_labels, dataset_name,\n",
    "                             model_name='gin_supervised_contextpred',\n",
    "                             save_path=save_results_to)\n",
    "\n",
    "logme = LogME(regression=False)\n",
    "score = logme.fit(np.array(imdbm_graph_features), np.array(imdbm_graph_labels))\n",
    "print(\"\\n=============\")\n",
    "print(\"logme score (GIN supervised_contextpred.pth): {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e3e65287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============\n",
      "logme score (GIN infomax.pth): -0.6333549741453175\n"
     ]
    }
   ],
   "source": [
    "imdbm_graph_features, imdbm_graph_labels = get_graph_features_labels_imdb(imdbm_train_loader, \n",
    "                                                                          gin_infomax_model,\n",
    "                                                                          seed)\n",
    "create_dataframe_save_to_csv(imdbm_graph_features, imdbm_graph_labels, dataset_name,\n",
    "                             model_name='gin_infomax',\n",
    "                             save_path=save_results_to)\n",
    "\n",
    "logme = LogME(regression=False)\n",
    "score = logme.fit(np.array(imdbm_graph_features), np.array(imdbm_graph_labels))\n",
    "print(\"\\n=============\")\n",
    "print(\"logme score (GIN infomax.pth): {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dfb7db0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-th feature | mean: -125.73597717285156 +- 447.0168762207031\n",
      "1-th feature | mean: -15571.361328125 +- 58989.79296875\n",
      "2-th feature | mean: -189.79635620117188 +- 675.7182006835938\n",
      "3-th feature | mean: -12.429853439331055 +- 51.592445373535156\n",
      "4-th feature | mean: -1676.006103515625 +- 6203.193359375\n",
      "5-th feature | mean: -26.136045455932617 +- 102.60440826416016\n",
      "6-th feature | mean: -47.533721923828125 +- 178.07022094726562\n",
      "7-th feature | mean: -36.1640510559082 +- 136.61399841308594\n",
      "8-th feature | mean: -23.610666275024414 +- 92.90879821777344\n",
      "9-th feature | mean: -23.610660552978516 +- 92.9087905883789\n",
      "10-th feature | mean: -40.48372268676758 +- 152.52830505371094\n",
      "11-th feature | mean: -59.103515625 +- 214.29075622558594\n"
     ]
    }
   ],
   "source": [
    "for i, feature in enumerate(imdbm_graph_features):\n",
    "    print('{}-th feature | mean: {} +- {}'.format(i, np.mean(feature), np.std(feature)))\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865d3da2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
