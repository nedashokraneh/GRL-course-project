{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric\n",
    "\n",
    "from torch_geometric.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from model import GNN, GNN_graphpred\n",
    "from splitters import random_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_geometric.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code copied from: https://github.com/pyg-team/pytorch_geometric/blob/90a06d02b79414fc48a3367c79cfb28f919d5769/torch_geometric/datasets/tu_dataset.py ## \n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset, download_url, extract_zip\n",
    "from torch_geometric.read import read_tu_data\n",
    "\n",
    "\n",
    "class TUDataset(InMemoryDataset):\n",
    "    r\"\"\"A variety of graph kernel benchmark datasets, *.e.g.* \"IMDB-BINARY\",\n",
    "    \"REDDIT-BINARY\" or \"PROTEINS\", collected from the `TU Dortmund University\n",
    "    <http://graphkernels.cs.tu-dortmund.de>`_.\n",
    "    Args:\n",
    "        root (string): Root directory where the dataset should be saved.\n",
    "        name (string): The `name <http://graphkernels.cs.tu-dortmund.de>`_ of\n",
    "            the dataset.\n",
    "        transform (callable, optional): A function/transform that takes in an\n",
    "            :obj:`torch_geometric.data.Data` object and returns a transformed\n",
    "            version. The data object will be transformed before every access.\n",
    "            (default: :obj:`None`)\n",
    "        pre_transform (callable, optional): A function/transform that takes in\n",
    "            an :obj:`torch_geometric.data.Data` object and returns a\n",
    "            transformed version. The data object will be transformed before\n",
    "            being saved to disk. (default: :obj:`None`)\n",
    "        pre_filter (callable, optional): A function that takes in an\n",
    "            :obj:`torch_geometric.data.Data` object and returns a boolean\n",
    "            value, indicating whether the data object should be included in the\n",
    "            final dataset. (default: :obj:`None`)\n",
    "        use_node_attr (bool, optional): If :obj:`True`, the dataset will\n",
    "            contain additional continuous node features (if present).\n",
    "            (default: :obj:`False`)\n",
    "    \"\"\"\n",
    "\n",
    "    url = 'https://ls11-www.cs.tu-dortmund.de/people/morris/' \\\n",
    "          'graphkerneldatasets'\n",
    "\n",
    "    def __init__(self,\n",
    "                 root,\n",
    "                 name,\n",
    "                 transform=None,\n",
    "                 pre_transform=None,\n",
    "                 pre_filter=None,\n",
    "                 use_node_attr=False):\n",
    "        self.name = name\n",
    "        super(TUDataset, self).__init__(root, transform, pre_transform,\n",
    "                                        pre_filter)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        if self.data.x is not None and not use_node_attr:\n",
    "            self.data.x = self.data.x[:, self.num_node_attributes:]\n",
    "\n",
    "    @property\n",
    "    def num_node_labels(self):\n",
    "        if self.data.x is None:\n",
    "            return 0\n",
    "\n",
    "        for i in range(self.data.x.size(1)):\n",
    "            if self.data.x[:, i:].sum().item() == self.data.x.size(0):\n",
    "                return self.data.x.size(1) - i\n",
    "\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def num_node_attributes(self):\n",
    "        if self.data.x is None:\n",
    "            return 0\n",
    "\n",
    "        return self.data.x.size(1) - self.num_node_labels\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        names = ['A', 'graph_indicator']\n",
    "        return ['{}_{}.txt'.format(self.name, name) for name in names]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return 'data.pt'\n",
    "\n",
    "    def download(self):\n",
    "        path = download_url('{}/{}.zip'.format(self.url, self.name), self.root)\n",
    "        extract_zip(path, self.root)\n",
    "        os.unlink(path)\n",
    "        shutil.rmtree(self.raw_dir)\n",
    "        os.rename(osp.join(self.root, self.name), self.raw_dir)\n",
    "\n",
    "    def process(self):\n",
    "        self.data, self.slices = read_tu_data(self.raw_dir, self.name)\n",
    "\n",
    "        if self.pre_filter is not None:\n",
    "            data_list = [self.get(idx) for idx in range(len(self))]\n",
    "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
    "            self.data, self.slices = self.collate(data_list)\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.get(idx) for idx in range(len(self))]\n",
    "            data_list = [self.pre_transform(data) for data in data_list]\n",
    "            self.data, self.slices = self.collate(data_list)\n",
    "\n",
    "        torch.save((self.data, self.slices), self.processed_paths[0])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({})'.format(self.name, len(self))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from https://github.com/thuml/LogME/blob/main/LogME.py\n",
    "## NOTE: commented out njit because that package takes forever to load in jupyter notebook\n",
    "\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "# from numba import njit\n",
    "\n",
    "# @njit\n",
    "def each_evidence(y_, f, fh, v, s, vh, N, D):\n",
    "    \"\"\"\n",
    "    compute the maximum evidence for each class\n",
    "    \"\"\"\n",
    "    epsilon = 1e-5\n",
    "    alpha = 1.0\n",
    "    beta = 1.0\n",
    "    lam = alpha / beta\n",
    "    tmp = (vh @ (f @ np.ascontiguousarray(y_)))\n",
    "    for _ in range(11):\n",
    "        # should converge after at most 10 steps\n",
    "        # typically converge after two or three steps\n",
    "        gamma = (s / (s + lam)).sum()\n",
    "        # A = v @ np.diag(alpha + beta * s) @ v.transpose() # no need to compute A\n",
    "        # A_inv = v @ np.diag(1.0 / (alpha + beta * s)) @ v.transpose() # no need to compute A_inv\n",
    "        m = v @ (tmp * beta / (alpha + beta * s))\n",
    "        alpha_de = (m * m).sum()\n",
    "        alpha = gamma / (alpha_de + epsilon)\n",
    "        beta_de = ((y_ - fh @ m) ** 2).sum()\n",
    "        beta = (N - gamma) / (beta_de + epsilon)\n",
    "        new_lam = alpha / beta\n",
    "        if np.abs(new_lam - lam) / lam < 0.01:\n",
    "            break\n",
    "        lam = new_lam\n",
    "    evidence = D / 2.0 * np.log(alpha) \\\n",
    "               + N / 2.0 * np.log(beta) \\\n",
    "               - 0.5 * np.sum(np.log(alpha + beta * s)) \\\n",
    "               - beta / 2.0 * (beta_de + epsilon) \\\n",
    "               - alpha / 2.0 * (alpha_de + epsilon) \\\n",
    "               - N / 2.0 * np.log(2 * np.pi)\n",
    "    return evidence / N, alpha, beta, m\n",
    "\n",
    "\n",
    "# use pseudo data to compile the function\n",
    "# D = 20, N = 50\n",
    "f_tmp = np.random.randn(20, 50).astype(np.float64)\n",
    "each_evidence(np.random.randint(0, 2, 50).astype(np.float64), f_tmp, f_tmp.transpose(), np.eye(20, dtype=np.float64), np.ones(20, dtype=np.float64), np.eye(20, dtype=np.float64), 50, 20)\n",
    "\n",
    "\n",
    "# @njit\n",
    "def truncated_svd(x):\n",
    "    u, s, vh = np.linalg.svd(x.transpose() @ x)\n",
    "    s = np.sqrt(s)\n",
    "    u_times_sigma = x @ vh.transpose()\n",
    "    k = np.sum((s > 1e-10) * 1)  # rank of f\n",
    "    s = s.reshape(-1, 1)\n",
    "    s = s[:k]\n",
    "    vh = vh[:k]\n",
    "    u = u_times_sigma[:, :k] / s.reshape(1, -1)\n",
    "    return u, s, vh\n",
    "truncated_svd(np.random.randn(20, 10).astype(np.float64))\n",
    "\n",
    "\n",
    "class LogME(object):\n",
    "    def __init__(self, regression=False):\n",
    "        \"\"\"\n",
    "            :param regression: whether regression\n",
    "        \"\"\"\n",
    "        self.regression = regression\n",
    "        self.fitted = False\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.num_dim = 0\n",
    "        self.alphas = []  # alpha for each class / dimension\n",
    "        self.betas = []  # beta for each class / dimension\n",
    "        # self.ms.shape --> [C, D]\n",
    "        self.ms = []  # m for each class / dimension\n",
    "\n",
    "    def _fit_icml(self, f: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        LogME calculation proposed in the ICML 2021 paper\n",
    "        \"LogME: Practical Assessment of Pre-trained Models for Transfer Learning\"\n",
    "        at http://proceedings.mlr.press/v139/you21b.html\n",
    "        \"\"\"\n",
    "        fh = f\n",
    "        f = f.transpose()\n",
    "        D, N = f.shape\n",
    "        v, s, vh = np.linalg.svd(f @ fh, full_matrices=True)\n",
    "\n",
    "        evidences = []\n",
    "        self.num_dim = y.shape[1] if self.regression else int(y.max() + 1)\n",
    "        for i in range(self.num_dim):\n",
    "            y_ = y[:, i] if self.regression else (y == i).astype(np.float64)\n",
    "            evidence, alpha, beta, m = each_evidence(y_, f, fh, v, s, vh, N, D)\n",
    "            evidences.append(evidence)\n",
    "            self.alphas.append(alpha)\n",
    "            self.betas.append(beta)\n",
    "            self.ms.append(m)\n",
    "        self.ms = np.stack(self.ms)\n",
    "        return np.mean(evidences)\n",
    "\n",
    "    def _fit_fixed_point(self, f: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        LogME calculation proposed in the arxiv 2021 paper\n",
    "        \"Ranking and Tuning Pre-trained Models: A New Paradigm of Exploiting Model Hubs\"\n",
    "        at https://arxiv.org/abs/2110.10545\n",
    "        \"\"\"\n",
    "        N, D = f.shape  # k = min(N, D)\n",
    "        if N > D: # direct SVD may be expensive\n",
    "            u, s, vh = truncated_svd(f)\n",
    "        else:\n",
    "            u, s, vh = np.linalg.svd(f, full_matrices=False)\n",
    "        # u.shape = N x k\n",
    "        # s.shape = k\n",
    "        # vh.shape = k x D\n",
    "        s = s.reshape(-1, 1)\n",
    "        sigma = (s ** 2)\n",
    "\n",
    "        evidences = []\n",
    "        self.num_dim = y.shape[1] if self.regression else int(y.max() + 1)\n",
    "        for i in range(self.num_dim):\n",
    "            y_ = y[:, i] if self.regression else (y == i).astype(np.float64)\n",
    "            y_ = y_.reshape(-1, 1)\n",
    "            x = u.T @ y_  # x has shape [k, 1], but actually x should have shape [N, 1]\n",
    "            x2 = x ** 2\n",
    "            res_x2 = (y_ ** 2).sum() - x2.sum()  # if k < N, we compute sum of xi for 0 singular values directly\n",
    "\n",
    "            alpha, beta = 1.0, 1.0\n",
    "            for _ in range(11):\n",
    "                t = alpha / beta\n",
    "                gamma = (sigma / (sigma + t)).sum()\n",
    "                m2 = (sigma * x2 / ((t + sigma) ** 2)).sum()\n",
    "                res2 = (x2 / ((1 + sigma / t) ** 2)).sum() + res_x2\n",
    "                alpha = gamma / (m2 + 1e-5)\n",
    "                beta = (N - gamma) / (res2 + 1e-5)\n",
    "                t_ = alpha / beta\n",
    "                evidence = D / 2.0 * np.log(alpha) \\\n",
    "                           + N / 2.0 * np.log(beta) \\\n",
    "                           - 0.5 * np.sum(np.log(alpha + beta * sigma)) \\\n",
    "                           - beta / 2.0 * res2 \\\n",
    "                           - alpha / 2.0 * m2 \\\n",
    "                           - N / 2.0 * np.log(2 * np.pi)\n",
    "                evidence /= N\n",
    "                if abs(t_ - t) / t <= 1e-3:  # abs(t_ - t) <= 1e-5 or abs(1 / t_ - 1 / t) <= 1e-5:\n",
    "                    break\n",
    "            evidence = D / 2.0 * np.log(alpha) \\\n",
    "                       + N / 2.0 * np.log(beta) \\\n",
    "                       - 0.5 * np.sum(np.log(alpha + beta * sigma)) \\\n",
    "                       - beta / 2.0 * res2 \\\n",
    "                       - alpha / 2.0 * m2 \\\n",
    "                       - N / 2.0 * np.log(2 * np.pi)\n",
    "            evidence /= N\n",
    "            m = 1.0 / (t + sigma) * s * x\n",
    "            m = (vh.T @ m).reshape(-1)\n",
    "            evidences.append(evidence)\n",
    "            self.alphas.append(alpha)\n",
    "            self.betas.append(beta)\n",
    "            self.ms.append(m)\n",
    "        self.ms = np.stack(self.ms)\n",
    "        return np.mean(evidences)\n",
    "\n",
    "    _fit = _fit_fixed_point\n",
    "\n",
    "    def fit(self, f: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        :param f: [N, F], feature matrix from pre-trained model\n",
    "        :param y: target labels.\n",
    "            For classification, y has shape [N] with element in [0, C_t).\n",
    "            For regression, y has shape [N, C] with C regression-labels\n",
    "        :return: LogME score (how well f can fit y directly)\n",
    "        \"\"\"\n",
    "        if self.fitted:\n",
    "            warnings.warn('re-fitting for new data. old parameters cleared.')\n",
    "            self.reset()\n",
    "        else:\n",
    "            self.fitted = True\n",
    "        f = f.astype(np.float64)\n",
    "        if self.regression:\n",
    "            y = y.astype(np.float64)\n",
    "            if len(y.shape) == 1:\n",
    "                y = y.reshape(-1, 1)\n",
    "        return self._fit(f, y)\n",
    "\n",
    "    def predict(self, f: np.ndarray):\n",
    "        \"\"\"\n",
    "        :param f: [N, F], feature matrix\n",
    "        :return: prediction, return shape [N, X]\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"not fitted, please call fit first\")\n",
    "        f = f.astype(np.float64)\n",
    "        logits = f @ self.ms.T\n",
    "        if self.regression:\n",
    "            return logits\n",
    "        return np.argmax(logits, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_features_labels_imdb(loader, model, seed):\n",
    "    \"\"\"Extract graph features for IMDB graph data.\n",
    "    Note: \n",
    "        Fake node features ([0, 0]) and fake edge attributes ([0, 0])\n",
    "        are created for each node and edge so we can use the pre-trained \n",
    "        Strategies (Hu et al., ICLR 2020) GNNs.\n",
    "        \n",
    "    Args:\n",
    "        loader : IMDB dataloader\n",
    "        model : pre-trained GNN model\n",
    "        seed : integer value random seed value\n",
    "    \n",
    "    Returns:\n",
    "        all_graph_features : list of all graph features from the dataloader\n",
    "        all_graph_labels : list of all graph labels from the dataloader \n",
    "    \"\"\"\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    all_graph_features = []\n",
    "    all_graph_labels = []\n",
    "    \n",
    "    for step, batch in enumerate(loader):\n",
    "#         print(\"step: {}\".format(step))\n",
    "#         print(\"batch: {}\".format(batch))\n",
    "#         print(\"batch.batch.shape: {}\".format(batch.batch.shape))\n",
    "        num_nodes = batch.batch.shape[0]\n",
    "        num_edges = batch.edge_index.shape[1]\n",
    "#         print(\"num_nodes: {}\".format(num_nodes))\n",
    "#         print(\"num_edges: {}\".format(num_edges))\n",
    "        batch = batch.to(device)\n",
    "    \n",
    "        # create fake node features [0, 0] for each node of the IMDB dataset\n",
    "        x = torch.zeros(size=[num_nodes, 2], dtype=torch.long).to(device)\n",
    "\n",
    "        # create fake edge attribute [0, 0] for each edge of the IMDB dataset\n",
    "        edge_attr = torch.zeros(size=[num_edges, 2], dtype=torch.long).to(device)\n",
    "\n",
    "        y = batch.y\n",
    "        edge_index = batch.edge_index\n",
    "        batch = batch.batch\n",
    "\n",
    "        node_representation = model.gnn(x, edge_index, edge_attr)\n",
    "\n",
    "        graph_features = model.pool(node_representation, batch)\n",
    "#         print(\"graph_features: {}\".format(graph_features))\n",
    "#         print(\"graph_features.shape: {}\".format(graph_features.shape)) # batch_size x outdim (300)\n",
    "        all_graph_features.extend(graph_features.cpu().detach().numpy())\n",
    "        all_graph_labels.extend(y.cpu().detach().numpy())\n",
    "        \n",
    "    return all_graph_features, all_graph_labels\n",
    "\n",
    "\n",
    "def create_dataframe_save_to_csv(embeddings, labels, dataset_name, model_name, save_path):\n",
    "#     ## create pandas dataframe to store: example id, embeddings, labels ## \n",
    "#     d = {'example_id': [i for i in range(len(embeddings))],\n",
    "#             'embeddings': embeddings,\n",
    "#             'labels': labels\n",
    "#            }\n",
    "#     df = pd.DataFrame(data=d)\n",
    "#     # df.head(15)\n",
    "\n",
    "#     if not os.path.exists(save_path):\n",
    "#         os.makedirs(save_path)\n",
    "\n",
    "    filename = '{}_{}.csv'.format(dataset_name, model_name)\n",
    "#     print(\"dataset_name: {}\".format(dataset_name))\n",
    "#     df.to_csv(os.path.join(save_path, filename), index=False)\n",
    "    emb_df = pd.DataFrame(np.array(embeddings))\n",
    "    emb_df.columns = ['emb' + str(e+1) for e in range(emb_df.shape[1])]\n",
    "    emb_df['label'] = labels\n",
    "#     print(\"emb_df: \\n{}\\n\".format(emb_df))\n",
    "    emb_df.to_csv(os.path.join(save_path, filename), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GNN default values ## \n",
    "num_layer = 5 # default\n",
    "emb_dim = 300 # default\n",
    "JK = 'last' # default (how the node features across laysers are combined)\n",
    "dropout_ratio = 0.5 # default\n",
    "graph_pooling = 'mean' # default\n",
    "gnn_type = 'gin' # default\n",
    "\n",
    "## DataLoader default values ## \n",
    "batch_size = 32 # strategies default\n",
    "num_workers = 4 # strategies default\n",
    "train_shuffle = False \n",
    "\n",
    "num_tasks = 1\n",
    "\n",
    "\n",
    "## others ## \n",
    "save_results_to = '/mnt/sdc/course-projects/GRL-course-project/results'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GNN_graphpred(\n",
       "  (gnn): GNN(\n",
       "    (x_embedding1): Embedding(120, 300)\n",
       "    (x_embedding2): Embedding(3, 300)\n",
       "    (gnns): ModuleList(\n",
       "      (0): GINConv(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=600, out_features=300, bias=True)\n",
       "        )\n",
       "        (edge_embedding1): Embedding(6, 300)\n",
       "        (edge_embedding2): Embedding(3, 300)\n",
       "      )\n",
       "      (1): GINConv(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=600, out_features=300, bias=True)\n",
       "        )\n",
       "        (edge_embedding1): Embedding(6, 300)\n",
       "        (edge_embedding2): Embedding(3, 300)\n",
       "      )\n",
       "      (2): GINConv(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=600, out_features=300, bias=True)\n",
       "        )\n",
       "        (edge_embedding1): Embedding(6, 300)\n",
       "        (edge_embedding2): Embedding(3, 300)\n",
       "      )\n",
       "      (3): GINConv(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=600, out_features=300, bias=True)\n",
       "        )\n",
       "        (edge_embedding1): Embedding(6, 300)\n",
       "        (edge_embedding2): Embedding(3, 300)\n",
       "      )\n",
       "      (4): GINConv(\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=600, out_features=300, bias=True)\n",
       "        )\n",
       "        (edge_embedding1): Embedding(6, 300)\n",
       "        (edge_embedding2): Embedding(3, 300)\n",
       "      )\n",
       "    )\n",
       "    (batch_norms): ModuleList(\n",
       "      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Set up model ## \n",
    "model = GNN_graphpred(num_layer, emb_dim, num_tasks, JK, dropout_ratio, graph_pooling, gnn_type)\n",
    "\n",
    "##########################\n",
    "input_model_file = './model_gin/supervised.pth'\n",
    "\n",
    "gin_supervised_model = GNN_graphpred(num_layer, emb_dim, num_tasks, JK = JK, drop_ratio = dropout_ratio, graph_pooling = graph_pooling, gnn_type = gnn_type)\n",
    "gin_supervised_model.from_pretrained(input_model_file)\n",
    "\n",
    "gin_supervised_model.to(device)\n",
    "gin_supervised_model.eval()\n",
    "\n",
    "###########################\n",
    "\n",
    "input_model_file = './model_gin/supervised_infomax.pth'\n",
    "\n",
    "gin_supervised_infomax_model = GNN_graphpred(num_layer, emb_dim, num_tasks, JK = JK, drop_ratio = dropout_ratio, graph_pooling = graph_pooling, gnn_type = gnn_type)\n",
    "gin_supervised_infomax_model.from_pretrained(input_model_file)\n",
    "\n",
    "gin_supervised_infomax_model.to(device)\n",
    "gin_supervised_infomax_model.eval()\n",
    "\n",
    "###########################\n",
    "\n",
    "input_model_file = './model_gin/supervised_edgepred.pth'\n",
    "\n",
    "gin_supervised_edgepred_model = GNN_graphpred(num_layer, emb_dim, num_tasks, JK = JK, drop_ratio = dropout_ratio, graph_pooling = graph_pooling, gnn_type = gnn_type)\n",
    "gin_supervised_edgepred_model.from_pretrained(input_model_file)\n",
    "\n",
    "gin_supervised_edgepred_model.to(device)\n",
    "gin_supervised_edgepred_model.eval()\n",
    "\n",
    "###########################\n",
    "\n",
    "input_model_file = './model_gin/supervised_masking.pth'\n",
    "\n",
    "gin_supervised_masking_model = GNN_graphpred(num_layer, emb_dim, num_tasks, JK = JK, drop_ratio = dropout_ratio, graph_pooling = graph_pooling, gnn_type = gnn_type)\n",
    "gin_supervised_masking_model.from_pretrained(input_model_file)\n",
    "\n",
    "gin_supervised_masking_model.to(device)\n",
    "gin_supervised_masking_model.eval()\n",
    "\n",
    "###########################\n",
    "\n",
    "input_model_file = './model_gin/supervised_contextpred.pth'\n",
    "\n",
    "gin_supervised_contextpred_model = GNN_graphpred(num_layer, emb_dim, num_tasks, JK = JK, drop_ratio = dropout_ratio, graph_pooling = graph_pooling, gnn_type = gnn_type)\n",
    "gin_supervised_contextpred_model.from_pretrained(input_model_file)\n",
    "\n",
    "gin_supervised_contextpred_model.to(device)\n",
    "gin_supervised_contextpred_model.eval()\n",
    "\n",
    "###########################\n",
    "\n",
    "input_model_file = './model_gin/infomax.pth'\n",
    "\n",
    "gin_infomax_model = GNN_graphpred(num_layer, emb_dim, num_tasks, JK = JK, drop_ratio = dropout_ratio, graph_pooling = graph_pooling, gnn_type = gnn_type)\n",
    "gin_infomax_model.from_pretrained(input_model_file)\n",
    "\n",
    "gin_infomax_model.to(device)\n",
    "gin_infomax_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load IMDB Dataset + test using Strategies pre-trained GNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://ls11-www.cs.tu-dortmund.de/people/morris/graphkerneldatasets/IMDB-BINARY.zip\n",
      "Extracting data/imdb/binary/IMDB-BINARY.zip\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "## dataset downloaded to '/pretrain-gnns-master/chem/data/imdb/binary' ## \n",
    "dataset_name = 'imdbb'\n",
    "imdb_dataset = TUDataset(root='data/imdb/binary', name='IMDB-BINARY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 146], y=[1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdbb_train_dataset, imdbb_valid_dataset, imdbb_test_dataset = random_split(imdb_dataset, null_value=0, \n",
    "                                                                         frac_train=0.8,\n",
    "                                                                         frac_valid=0.1,\n",
    "                                                                         frac_test=0.1,\n",
    "                                                                         seed=seed)\n",
    "\n",
    "imdbb_train_loader = DataLoader(imdbb_train_dataset, batch_size=batch_size, \n",
    "                               shuffle=train_shuffle, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============\n",
      "logme score (GIN supervised.pth): -0.6136270376991717\n"
     ]
    }
   ],
   "source": [
    "\n",
    "imdbb_graph_features, imdbb_graph_labels = get_graph_features_labels_imdb(imdbb_train_loader, \n",
    "                                                                          gin_supervised_model,\n",
    "                                                                          seed)\n",
    "'''\n",
    "create_dataframe_save_to_csv(imdbb_graph_features, imdbb_graph_labels, dataset_name,\n",
    "                             model_name='gin_supervised',\n",
    "                             save_path=save_results_to)\n",
    "'''\n",
    "logme = LogME(regression=False)\n",
    "score = logme.fit(np.array(imdbb_graph_features), np.array(imdbb_graph_labels))\n",
    "print(\"\\n=============\")\n",
    "print(\"logme score (GIN supervised.pth): {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============\n",
      "logme score (GIN supervised_infomax.pth): -0.62494134417472\n"
     ]
    }
   ],
   "source": [
    "imdbb_graph_features, imdbb_graph_labels = get_graph_features_labels_imdb(imdbb_train_loader, \n",
    "                                                                          gin_supervised_infomax_model,\n",
    "                                                                          seed)\n",
    "create_dataframe_save_to_csv(imdbb_graph_features, imdbb_graph_labels, dataset_name,\n",
    "                             model_name='gin_supervised_infomax',\n",
    "                             save_path=save_results_to)\n",
    "\n",
    "logme = LogME(regression=False)\n",
    "score = logme.fit(np.array(imdbb_graph_features), np.array(imdbb_graph_labels))\n",
    "print(\"\\n=============\")\n",
    "print(\"logme score (GIN supervised_infomax.pth): {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
<<<<<<< HEAD
=======
   "id": "f57a31ec",
>>>>>>> 021dc0200ad8a095189e59f06ca3e7a8133a3d6b
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============\n",
      "logme score (GIN supervised_edgepred.pth): -0.6278003221418832\n"
     ]
    }
   ],
   "source": [
    "imdbb_graph_features, imdbb_graph_labels = get_graph_features_labels_imdb(imdbb_train_loader, \n",
    "                                                                          gin_supervised_edgepred_model,\n",
    "                                                                          seed)\n",
    "create_dataframe_save_to_csv(imdbb_graph_features, imdbb_graph_labels, dataset_name,\n",
    "                             model_name='gin_supervised_edgepred',\n",
    "                             save_path=save_results_to)\n",
    "\n",
    "logme = LogME(regression=False)\n",
    "score = logme.fit(np.array(imdbb_graph_features), np.array(imdbb_graph_labels))\n",
    "print(\"\\n=============\")\n",
    "print(\"logme score (GIN supervised_edgepred.pth): {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
<<<<<<< HEAD
=======
   "id": "7ac4f00b",
>>>>>>> 021dc0200ad8a095189e59f06ca3e7a8133a3d6b
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============\n",
      "logme score (GIN supervised_masking.pth): -0.6408088293272055\n"
     ]
    }
   ],
   "source": [
    "imdbb_graph_features, imdbb_graph_labels = get_graph_features_labels_imdb(imdbb_train_loader, \n",
    "                                                                          gin_supervised_masking_model,\n",
    "                                                                          seed)\n",
    "create_dataframe_save_to_csv(imdbb_graph_features, imdbb_graph_labels, dataset_name,\n",
    "                             model_name='gin_supervised_masking',\n",
    "                             save_path=save_results_to)\n",
    "\n",
    "logme = LogME(regression=False)\n",
    "score = logme.fit(np.array(imdbb_graph_features), np.array(imdbb_graph_labels))\n",
    "print(\"\\n=============\")\n",
    "print(\"logme score (GIN supervised_masking.pth): {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
<<<<<<< HEAD
=======
   "id": "070a86f3",
>>>>>>> 021dc0200ad8a095189e59f06ca3e7a8133a3d6b
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============\n",
      "logme score (GIN supervised_contextpred.pth): -0.6355372872939185\n"
     ]
    }
   ],
   "source": [
    "imdbb_graph_features, imdbb_graph_labels = get_graph_features_labels_imdb(imdbb_train_loader, \n",
    "                                                                          gin_supervised_contextpred_model,\n",
    "                                                                          seed)\n",
    "create_dataframe_save_to_csv(imdbb_graph_features, imdbb_graph_labels, dataset_name,\n",
    "                             model_name='gin_supervised_contextpred',\n",
    "                             save_path=save_results_to)\n",
    "\n",
    "logme = LogME(regression=False)\n",
    "score = logme.fit(np.array(imdbb_graph_features), np.array(imdbb_graph_labels))\n",
    "print(\"\\n=============\")\n",
    "print(\"logme score (GIN supervised_contextpred.pth): {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
<<<<<<< HEAD
=======
   "id": "d3026274",
>>>>>>> 021dc0200ad8a095189e59f06ca3e7a8133a3d6b
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============\n",
      "logme score (GIN infomax.pth): -0.6333544221479612\n"
     ]
    }
   ],
   "source": [
    "imdbb_graph_features, imdbb_graph_labels = get_graph_features_labels_imdb(imdbb_train_loader, \n",
    "                                                                          gin_infomax_model,\n",
    "                                                                          seed)\n",
    "create_dataframe_save_to_csv(imdbb_graph_features, imdbb_graph_labels, dataset_name,\n",
    "                             model_name='gin_infomax',\n",
    "                             save_path=save_results_to)\n",
    "\n",
    "logme = LogME(regression=False)\n",
    "score = logme.fit(np.array(imdbb_graph_features), np.array(imdbb_graph_labels))\n",
    "print(\"\\n=============\")\n",
    "print(\"logme score (GIN infomax.pth): {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
=======
   "id": "6390e989",
>>>>>>> 021dc0200ad8a095189e59f06ca3e7a8133a3d6b
   "metadata": {},
   "source": [
    "## IMDB Multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
<<<<<<< HEAD
=======
   "id": "e2a42cf0",
>>>>>>> 021dc0200ad8a095189e59f06ca3e7a8133a3d6b
   "metadata": {},
   "outputs": [],
   "source": [
    "## dataset downloaded to '/pretrain-gnns-master/chem/data/imdb/binary' ## \n",
    "dataset_name = 'imdbm'\n",
    "imdb_dataset = TUDataset(root='data/imdb/multi', name='IMDB-MULTI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
<<<<<<< HEAD
=======
   "id": "d314ba58",
>>>>>>> 021dc0200ad8a095189e59f06ca3e7a8133a3d6b
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset\n",
    "imdb_m_labels = [i.y.item() for i in imdb_dataset]\n",
    "np.unique(imdb_m_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
<<<<<<< HEAD
=======
   "id": "84f34eb7",
>>>>>>> 021dc0200ad8a095189e59f06ca3e7a8133a3d6b
   "metadata": {},
   "outputs": [],
   "source": [
    "imdbm_train_dataset, imdbm_valid_dataset, imdbm_test_dataset = random_split(imdb_dataset, null_value=0, \n",
    "                                                                         frac_train=0.8,\n",
    "                                                                         frac_valid=0.1,\n",
    "                                                                         frac_test=0.1,\n",
    "                                                                         seed=seed)\n",
    "\n",
    "imdbm_train_loader = DataLoader(imdbm_train_dataset, batch_size=batch_size, \n",
    "                               shuffle=train_shuffle, num_workers=num_workers)\n",
    "\n",
    "# for batch in imdbm_train_loader:\n",
    "#     y = batch.y.cpu().detach().numpy()\n",
    "#     print(\"y: {}\".format(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
<<<<<<< HEAD
=======
   "id": "4778a13c",
>>>>>>> 021dc0200ad8a095189e59f06ca3e7a8133a3d6b
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============\n",
      "logme score (GIN supervised.pth): -0.6487061538629875\n"
     ]
    }
   ],
   "source": [
    "imdbm_graph_features, imdbm_graph_labels = get_graph_features_labels_imdb(imdbm_train_loader, \n",
    "                                                                          gin_supervised_model,\n",
    "                                                                          seed)\n",
    "create_dataframe_save_to_csv(imdbm_graph_features, imdbm_graph_labels, dataset_name,\n",
    "                             model_name='gin_supervised',\n",
    "                             save_path=save_results_to)\n",
    "\n",
    "logme = LogME(regression=False)\n",
    "score = logme.fit(np.array(imdbm_graph_features), np.array(imdbm_graph_labels))\n",
    "print(\"\\n=============\")\n",
    "print(\"logme score (GIN supervised.pth): {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
<<<<<<< HEAD
=======
   "id": "24c195ae",
>>>>>>> 021dc0200ad8a095189e59f06ca3e7a8133a3d6b
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============\n",
      "logme score (GIN supervised_infomax.pth): -0.6653021503430518\n"
     ]
    }
   ],
   "source": [
    "imdbm_graph_features, imdbm_graph_labels = get_graph_features_labels_imdb(imdbm_train_loader, \n",
    "                                                                          gin_supervised_infomax_model,\n",
    "                                                                          seed)\n",
    "create_dataframe_save_to_csv(imdbm_graph_features, imdbm_graph_labels, dataset_name,\n",
    "                             model_name='gin_supervised_infomax',\n",
    "                             save_path=save_results_to)\n",
    "\n",
    "logme = LogME(regression=False)\n",
    "score = logme.fit(np.array(imdbm_graph_features), np.array(imdbm_graph_labels))\n",
    "print(\"\\n=============\")\n",
    "print(\"logme score (GIN supervised_infomax.pth): {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
<<<<<<< HEAD
=======
   "id": "d2e848b8",
>>>>>>> 021dc0200ad8a095189e59f06ca3e7a8133a3d6b
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============\n",
      "logme score (GIN supervised_edgepred.pth): -0.6630349389346405\n"
     ]
    }
   ],
   "source": [
    "imdbm_graph_features, imdbm_graph_labels = get_graph_features_labels_imdb(imdbm_train_loader, \n",
    "                                                                          gin_supervised_edgepred_model,\n",
    "                                                                          seed)\n",
    "create_dataframe_save_to_csv(imdbm_graph_features, imdbm_graph_labels, dataset_name,\n",
    "                             model_name='gin_supervised_edgepred',\n",
    "                             save_path=save_results_to)\n",
    "\n",
    "logme = LogME(regression=False)\n",
    "score = logme.fit(np.array(imdbm_graph_features), np.array(imdbm_graph_labels))\n",
    "print(\"\\n=============\")\n",
    "print(\"logme score (GIN supervised_edgepred.pth): {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
<<<<<<< HEAD
=======
   "id": "6361e78f",
>>>>>>> 021dc0200ad8a095189e59f06ca3e7a8133a3d6b
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============\n",
      "logme score (GIN supervised_masking.pth): -0.6714810389185831\n"
     ]
    }
   ],
   "source": [
    "imdbm_graph_features, imdbm_graph_labels = get_graph_features_labels_imdb(imdbm_train_loader, \n",
    "                                                                          gin_supervised_masking_model,\n",
    "                                                                          seed)\n",
    "create_dataframe_save_to_csv(imdbm_graph_features, imdbm_graph_labels, dataset_name,\n",
    "                             model_name='gin_supervised_masking',\n",
    "                             save_path=save_results_to)\n",
    "\n",
    "logme = LogME(regression=False)\n",
    "score = logme.fit(np.array(imdbm_graph_features), np.array(imdbm_graph_labels))\n",
    "print(\"\\n=============\")\n",
    "print(\"logme score (GIN supervised_masking.pth): {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
<<<<<<< HEAD
=======
   "id": "7e594f4c",
>>>>>>> 021dc0200ad8a095189e59f06ca3e7a8133a3d6b
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============\n",
      "logme score (GIN supervised_contextpred.pth): -0.6716127862199371\n"
     ]
    }
   ],
   "source": [
    "imdbm_graph_features, imdbm_graph_labels = get_graph_features_labels_imdb(imdbm_train_loader, \n",
    "                                                                          gin_supervised_contextpred_model,\n",
    "                                                                          seed)\n",
    "create_dataframe_save_to_csv(imdbm_graph_features, imdbm_graph_labels, dataset_name,\n",
    "                             model_name='gin_supervised_contextpred',\n",
    "                             save_path=save_results_to)\n",
    "\n",
    "logme = LogME(regression=False)\n",
    "score = logme.fit(np.array(imdbm_graph_features), np.array(imdbm_graph_labels))\n",
    "print(\"\\n=============\")\n",
    "print(\"logme score (GIN supervised_contextpred.pth): {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
<<<<<<< HEAD
=======
   "id": "fbe5ce0e",
>>>>>>> 021dc0200ad8a095189e59f06ca3e7a8133a3d6b
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============\n",
      "logme score (GIN infomax.pth): -0.6680537971518762\n"
     ]
    }
   ],
   "source": [
    "imdbm_graph_features, imdbm_graph_labels = get_graph_features_labels_imdb(imdbm_train_loader, \n",
    "                                                                          gin_infomax_model,\n",
    "                                                                          seed)\n",
    "create_dataframe_save_to_csv(imdbm_graph_features, imdbm_graph_labels, dataset_name,\n",
    "                             model_name='gin_infomax',\n",
    "                             save_path=save_results_to)\n",
    "\n",
    "logme = LogME(regression=False)\n",
    "score = logme.fit(np.array(imdbm_graph_features), np.array(imdbm_graph_labels))\n",
    "print(\"\\n=============\")\n",
    "print(\"logme score (GIN infomax.pth): {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
<<<<<<< HEAD
=======
   "id": "914a953c",
>>>>>>> 021dc0200ad8a095189e59f06ca3e7a8133a3d6b
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(imdbm_graph_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
<<<<<<< HEAD
=======
   "id": "c7e106ae",
>>>>>>> 021dc0200ad8a095189e59f06ca3e7a8133a3d6b
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB-B\n",
      "0-th feature | mean: -125.73597717285156 +- 447.0168762207031\n",
      "1-th feature | mean: -15571.361328125 +- 58989.79296875\n",
      "2-th feature | mean: -189.79635620117188 +- 675.7182006835938\n",
      "3-th feature | mean: -12.429856300354004 +- 51.592445373535156\n",
      "4-th feature | mean: -1676.006103515625 +- 6203.193359375\n",
      "5-th feature | mean: -26.136045455932617 +- 102.60440826416016\n",
      "6-th feature | mean: -47.53371047973633 +- 178.07020568847656\n",
      "7-th feature | mean: -36.1640510559082 +- 136.614013671875\n",
      "8-th feature | mean: -23.61066246032715 +- 92.90879821777344\n",
      "9-th feature | mean: -23.610660552978516 +- 92.9087905883789\n",
      "10-th feature | mean: -40.48373031616211 +- 152.52830505371094\n",
      "11-th feature | mean: -59.1035270690918 +- 214.29075622558594\n",
      "\n",
      "IMDB-M\n",
      "0-th feature | mean: -9.032389640808105 +- 38.596763610839844\n",
      "1-th feature | mean: -47.261192321777344 +- 179.49412536621094\n",
      "2-th feature | mean: -47.261192321777344 +- 179.49412536621094\n",
      "3-th feature | mean: -0.49941253662109375 +- 1.8813073635101318\n",
      "4-th feature | mean: -12.429855346679688 +- 51.59244155883789\n",
      "5-th feature | mean: -303.1038818359375 +- 1092.3671875\n",
      "6-th feature | mean: -424.275634765625 +- 1540.7252197265625\n",
      "7-th feature | mean: -4.321245193481445 +- 19.64833641052246\n",
      "8-th feature | mean: -146.48196411132812 +- 522.7644653320312\n",
      "9-th feature | mean: -9.032389640808105 +- 38.596763610839844\n",
      "10-th feature | mean: -126.41722869873047 +- 449.1561279296875\n",
      "11-th feature | mean: -9.032389640808105 +- 38.596763610839844\n"
     ]
    }
   ],
   "source": [
    "print(\"IMDB-B\")\n",
    "for i, feature in enumerate(imdbb_graph_features):\n",
    "    print('{}-th feature | mean: {} +- {}'.format(i, np.mean(feature), np.std(feature)))\n",
    "    if i > 10:\n",
    "        break\n",
    "\n",
    "print(\"\\nIMDB-M\")\n",
    "for i, feature in enumerate(imdbm_graph_features):\n",
    "    print('{}-th feature | mean: {} +- {}'.format(i, np.mean(feature), np.std(feature)))\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
=======
   "id": "b35dae5f",
>>>>>>> 021dc0200ad8a095189e59f06ca3e7a8133a3d6b
   "metadata": {},
   "source": [
    "# Finetune Strategies pre-trained GNNs on IMDB datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
<<<<<<< HEAD
=======
   "id": "770e8499",
>>>>>>> 021dc0200ad8a095189e59f06ca3e7a8133a3d6b
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy code from chem/finetune.py"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
=======
   "id": "e1a842da",
>>>>>>> 021dc0200ad8a095189e59f06ca3e7a8133a3d6b
   "metadata": {},
   "source": [
    "## IMDB-B"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 29,
=======
   "execution_count": 32,
   "id": "3e6efc90",
>>>>>>> 021dc0200ad8a095189e59f06ca3e7a8133a3d6b
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.1.post2'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import argparse\n",
    "\n",
    "# from loader import MoleculeDataset\n",
    "# from torch_geometric.data import DataLoader\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "\n",
    "# from tqdm import tqdm\n",
    "# import numpy as np\n",
    "\n",
    "# from model import GNN, GNN_graphpred\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# from splitters import scaffold_split\n",
    "# import pandas as pd\n",
    "\n",
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# from tensorboardX import SummaryWriter\n",
    "\n",
    "# criterion = nn.BCEWithLogitsLoss(reduction = \"none\")\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3af4216",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37grl",
   "language": "python",
   "name": "py37grl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
